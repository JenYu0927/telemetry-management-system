image:
  repository: gcr.io/google-containers/fluentd-elasticsearch
## Specify an imagePullPolicy (Required)
## It's recommended to change this to 'Always' if the image tag is 'latest'
## ref: http://kubernetes.io/docs/user-guide/images/#updating-images
  tag: v2.3.2
  pullPolicy: IfNotPresent

## Configure resource requests and limits
## ref: http://kubernetes.io/docs/user-guide/compute-resources/
##
qct:
  node: 'worker-3'

resources:
  limits:
    cpu: 500m
    memory: 500Mi
  requests:
    cpu: 500m
    memory: 500Mi

elasticsearch:
  host: 'elasticsearch-discovery.default.svc.cluster.local'
  port: 9200
  scheme: 'http'
  ssl_version: TLSv1_2
  buffer_chunk_limit: 512M
  buffer_queue_limit: 8
  logstash_prefix: '5g-core'

# If you want to add custom environment variables, use the env dict
# You can then reference these in your config file e.g.:
#     user "#{ENV['OUTPUT_USER']}"
env:
  # OUTPUT_USER: my_user

# If you want to add custom environment variables from secrets, use the secret list
secret:
# - name: ELASTICSEARCH_PASSWORD
#   secret_name: elasticsearch
#   secret_key: password

rbac:
  create: true

serviceAccount:
  # Specifies whether a ServiceAccount should be created
  create: true
  # The name of the ServiceAccount to use.
  # If not set and create is true, a name is generated using the fullname template
  name:

## Specify if a Pod Security Policy for node-exporter must be created
## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/
##
podSecurityPolicy:
  enabled: false
  annotations: {}
    ## Specify pod annotations
    ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor
    ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp
    ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl
    ##
    # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'
    # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'
    # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'

livenessProbe:
  enabled: false

annotations: {}

podAnnotations: {}
  # prometheus.io/scrape: "true"
  # prometheus.io/port: "24231"

## DaemonSet update strategy
## Ref: https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/
updateStrategy:
  type: RollingUpdate

tolerations: {}
  # - key: node-role.kubernetes.io/master
  #   operator: Exists
  #   effect: NoSchedule

nodeSelector: {}

service: {}
  # type: ClusterIP
  # ports:
  #   - name: "monitor-agent"
  #     port: 24231

#service:
  # type: NodePort
  # ports:
  #   - name: "fluentd"
  #     port: 514
  #     nodePort: 30514
  #     protocol: UDP

configMaps:
  syslog.conf: |-
    <source>
      @type tail
      path /var/log/messages
      pos_file /var/log/messages.pos
      tag messages.log
      format /^(?<logtime>[^ ]*\s*[^ ]* [^ ]*) (?<host>[^ ]*) (?<ident>[a-zA-Z0-9_\/\.\-]*)(?:\[(?<pid>[0-9]+)\])?(?:[^\:]*\:)? *(?<message>.*)$/
      #format /^(?<time>[^ ]*\s*[^ ]* [^ ]*) (?<host>[^ ]*) (?<ident>[a-zA-Z0-9_\/\.\-]*)(?:\[(?<pid>[0-9]+)\])?(?:[^\:]*\:)? *(?<message>.*)$/
      read_from_head false
    </source>

    <filter messages.log>
      @type record_transformer
      <record>
        datacenter_id "30"
        room_id "31"
        rack_id "32"
        server_id "40"
      </record>
    </filter>

    <match messages.log>
      @id system.log
      @type elasticsearch
      include_tag_key true
      host "#{ENV['OUTPUT_HOST']}"
      port "#{ENV['OUTPUT_PORT']}"
      #scheme "#{ENV['OUTPUT_SCHEME']}"
      #ssl_version "#{ENV['OUTPUT_SSL_VERSION']}"
      logstash_format true
      logstash_prefix "syslog"
      reconnect_on_error true
      format multiline
      include_timestamp true
      time_format %Y-%m-%d %H:%M:%S
      type_name log
    </match>

  system.conf: |-
    <system>
      root_dir /tmp/fluentd-buffers/
      log_level info
    </system>
  #containers.input.conf: |-
    # This configuration file for Fluentd / td-agent is used
    # to watch changes to Docker log files. The kubelet creates symlinks that
    # capture the pod name, namespace, container name & Docker container ID
    # to the docker logs for pods in the /var/log/containers directory on the host.
    # If running this fluentd configuration in a Docker container, the /var/log
    # directory should be mounted in the container.
    #
    # These logs are then submitted to Elasticsearch which assumes the
    # installation of the fluent-plugin-elasticsearch & the
    # fluent-plugin-kubernetes_metadata_filter plugins.
    # See https://github.com/uken/fluent-plugin-elasticsearch &
    # https://github.com/fabric8io/fluent-plugin-kubernetes_metadata_filter for
    # more information about the plugins.
    #
    # Example
    # =======
    # A line in the Docker log file might look like this JSON:
    #
    # {"log":"2014/09/25 21:15:03 Got request with path wombat\n",
    #  "stream":"stderr",
    #   "time":"2014-09-25T21:15:03.499185026Z"}
    #
    # The time_format specification below makes sure we properly
    # parse the time format produced by Docker. This will be
    # submitted to Elasticsearch and should appear like:
    # $ curl 'http://elasticsearch-logging:9200/_search?pretty'
    # ...
    # {
    #      "_index" : "logstash-2014.09.25",
    #      "_type" : "fluentd",
    #      "_id" : "VBrbor2QTuGpsQyTCdfzqA",
    #      "_score" : 1.0,
    #      "_source":{"log":"2014/09/25 22:45:50 Got request with path wombat\n",
    #                 "stream":"stderr","tag":"docker.container.all",
    #                 "@timestamp":"2014-09-25T22:45:50+00:00"}
    #    },
    # ...
    #
    # The Kubernetes fluentd plugin is used to write the Kubernetes metadata to the log
    # record & add labels to the log record if properly configured. This enables users
    # to filter & search logs on any metadata.
    # For example a Docker container's logs might be in the directory:
    #
    #  /var/lib/docker/containers/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b
    #
    # and in the file:
    #
    #  997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b-json.log
    #
    # where 997599971ee6... is the Docker ID of the running container.
    # The Kubernetes kubelet makes a symbolic link to this file on the host machine
    # in the /var/log/containers directory which includes the pod name and the Kubernetes
    # container name:
    #
    #    synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log
    #    ->
    #    /var/lib/docker/containers/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b-json.log
    #
    # The /var/log directory on the host is mapped to the /var/log directory in the container
    # running this instance of Fluentd and we end up collecting the file:
    #
    #   /var/log/containers/synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log
    #
    # This results in the tag:
    #
    #  var.log.containers.synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log
    #
    # The Kubernetes fluentd plugin is used to extract the namespace, pod name & container name
    # which are added to the log message as a kubernetes field object & the Docker container ID
    # is also added under the docker field object.
    # The final tag is:
    #
    #   kubernetes.var.log.containers.synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log
    #
    # And the final log record look like:
    #
    # {
    #   "log":"2014/09/25 21:15:03 Got request with path wombat\n",
    #   "stream":"stderr",
    #   "time":"2014-09-25T21:15:03.499185026Z",
    #   "kubernetes": {
    #     "namespace": "default",
    #     "pod_name": "synthetic-logger-0.25lps-pod",
    #     "container_name": "synth-lgr"
    #   },
    #   "docker": {
    #     "container_id": "997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b"
    #   }
    # }
    #
    # This makes it easier for users to search for logs by pod name or by
    # the name of the Kubernetes container regardless of how many times the
    # Kubernetes pod has been restarted (resulting in a several Docker container IDs).
    # Json Log Example:
    # {"log":"[info:2016-02-16T16:04:05.930-08:00] Some log text here\n","stream":"stdout","time":"2016-02-17T00:04:05.931087621Z"}
    # CRI Log Example:
    # 2016-02-17T00:04:05.931087621Z stdout F [info:2016-02-16T16:04:05.930-08:00] Some log text here
    ##<source>
    ##  @id fluentd-containers.log
    ##  @type tail
    ##  path /var/log/containers/*.log
    ##  pos_file /var/log/fluentd/fluentd-containers.log.pos
    ##  time_format %Y-%m-%dT%H:%M:%S.%NZ
    ##  tag raw.kubernetes.*
    ##  #format json
    ##  read_from_head false # true
    ##  #Ben
    ##  <parse>
    ##    @type multi_format
    ##    <pattern>
    ##      format json
    ##      time_format %Y-%m-%dT%H:%M:%S.%NZ
    ##    </pattern>
    ##    <pattern>
    ##      format /^(?<time>.+) (?<stream>stdout|stderr) [^ ]* (?<log>.*)$/
    ##      time_format %Y-%m-%dT%H:%M:%S.%N%:z
    ##    </pattern>
    ##  </parse>
    ##</source>
    ##<filter raw.kubernetes.**>
    ##  @type record_transformer
    ##  <record>
    ##    Severity "Info"
    ##    Source   "Openshift"
    ##    EventType "Containers"
    ##  </record>
    ##</filter>
    ### Detect exceptions in the log output and forward them as one log entry.
    ##<match raw.kubernetes.**>
    ##  @id raw.kubernetes
    ##  @type detect_exceptions
    ##  remove_tag_prefix raw
    ##  message log
    ##  stream stream
    ##  multiline_flush_interval 5
    ##  max_bytes 500000
    ##  max_lines 1000
    ##</match>
    ##<match kubernetes.**>
    ##  @id elasticsearch_pod
    ##  @type elasticsearch                                       
    ##  @log_level info                                           
    ##  include_tag_key true                                      
    ##  #type_name _doc                                            
    ##  host "#{ENV['OUTPUT_HOST']}"                              
    ##  port "#{ENV['OUTPUT_PORT']}"                              
    ##  scheme "#{ENV['OUTPUT_SCHEME']}"                          
    ##  ssl_version "#{ENV['OUTPUT_SSL_VERSION']}"                
    ##  logstash_format true                                      
    ##  logstash_prefix "#{ENV['LOGSTASH_PREFIX']}"               
    ##  reconnect_on_error true                                   
    ##  <buffer>                                                  
    ##    @type file                                              
    ##    path /var/log/fluentd/fluentd-buffers/kubernetes.system.pod.buffer    
    ##    flush_mode interval                                     
    ##    retry_type exponential_backoff                          
    ##    flush_thread_count 2                                    
    ##    flush_interval 5s                                       
    ##    retry_forever                                           
    ##    retry_max_interval 30                                   
    ##    chunk_limit_size "#{ENV['OUTPUT_BUFFER_CHUNK_LIMIT']}"  
    ##    queue_limit_length "#{ENV['OUTPUT_BUFFER_QUEUE_LIMIT']}"
    ##    overflow_action block                                   
    ##  </buffer>                                                 
    ##</match>

  #upf.input.conf: |-
  #  <source>
  #    @id vpp.host.log
  #    @type tail
  #    path /var/log/vpp/vpp.log
  #    pos_file /var/log/fluentd/vpp.log.pos
  #    time_format %Y%m%d-%H:%M:%S.%L
  #    format /^(?<time>[^ ]*) (?<level>[^ ]*) \[(?<hostname>.*):(?<pid>\d+):(?<tid>\d+):(?<funcname>.*):(?<line>[^\]]\d+)\] (?<message>.*)$/
  #    tag upf.host.log
  #    read_from_head true
  #  </source>
  #cp.input.conf: |-
  #  <source>
  #    @id amf.host.log
  #    @type tail
  #    path /var/log/amf/amf.log
  #    pos_file /var/log/fluentd/amf.log.pos
  #    time_format %Y-%m-%dT%H:%M:%S.%LZ
  #    format /^(?<instance>[^ ].+) (?<thread>[^ ]\d.*) (?<time>[^ ].*) (?<level>[^ ].*) (?<classname>.+)\.(?<line>\d+)\: (?<message>.*)$/
  #    tag amf.host.log
  #    read_from_head true
  #  </source>
  #  <source>
  #    @id smf.host.log
  #    @type tail
  #    path /var/log/smf/smf.log
  #    pos_file /var/log/fluentd/smf.log.pos
  #    time_format %Y-%m-%dT%H:%M:%S.%LZ
  #    format /^(?<instance>[^ ].+) (?<thread>[^ ]\d.*) (?<time>[^ ].*) (?<level>[^ ].*) (?<classname>.+)\.(?<line>\d+)\: (?<message>.*)$/
  #    tag smf.host.log
  #    read_from_head true
  #  </source>

  cp.input.smf.conf: |-
    <source>
      @id smf_host_log
      @type tail
      path /var/log/smf/smf.log
      #pos_file /var/log/fluentd/smf.log.pos
      pos_file /home/telemetry_buffer/fluentd/smf.log.pos
      time_format %Y-%m-%dT%H:%M:%S.%N%:z
      format /^(?<message>.*)$/
      tag smf.host.log
      read_from_head false
    </source>
    #<filter smf.host.log>
    #  @type record_transformer
    #  <record>
    #    Severity "Info"
    #    Source   "VNF"
    #    EventType "SMF"
    #  </record>
    #</filter>
    <filter smf.host.log>     
      @type kubernetes_metadata
    </filter>                  
    <match smf.host.log>
      @id 5g_core_smf_log
      @type elasticsearch
      @log_level info
      include_tag_key true
      type_name _doc
      host "#{ENV['OUTPUT_HOST']}"
      port "#{ENV['OUTPUT_PORT']}"
      scheme "#{ENV['OUTPUT_SCHEME']}"
      ssl_version "#{ENV['OUTPUT_SSL_VERSION']}"
      logstash_format true
      logstash_prefix "#{ENV['LOGSTASH_PREFIX']}"
      reconnect_on_error true
      <buffer>
        @type file
        #path /var/log/fluentd/fluentd-buffers/5g.core.smf.buffer
        path /home/telemetry_buffer/fluentd/fluentd-buffers/5g.core.smf.buffer
        flush_mode interval
        retry_type exponential_backoff
        flush_thread_count 2
        flush_interval 5s
        retry_forever
        retry_max_interval 30
        chunk_limit_size "#{ENV['OUTPUT_BUFFER_CHUNK_LIMIT']}"
        queue_limit_length "#{ENV['OUTPUT_BUFFER_QUEUE_LIMIT']}"
        overflow_action block
      </buffer>
    </match>

  cp.input.amf.conf: |-
    <source>
      @id amf_host_log
      @type tail
      path /var/log/amf/amf.log
      #pos_file /var/log/fluentd/amf.log.pos
      pos_file /home/telemetry_buffer/fluentd/amf.log.pos
      time_format %Y-%m-%dT%H:%M:%S.%N%:z # %Y-%m-%dT%H:%M:%S.%LZ
      format /^(?<message>.*)$/
      tag amf.host.log
      read_from_head false
    </source>
    #<filter amf.host.log>     
    #  @type record_transformer
    #  <record>                
    #    Severity "Info"       
    #    Source   "VNF"        
    #    EventType "AMF"       
    #  </record>               
    #</filter>                 
    <filter amf.host.log>      
      @type kubernetes_metadata
    </filter>                  
    <match amf.host.log>
      @id 5g_core_amf_log
      @type elasticsearch
      @log_level info
      include_tag_key true
      type_name _doc
      host "#{ENV['OUTPUT_HOST']}"
      port "#{ENV['OUTPUT_PORT']}"
      scheme "#{ENV['OUTPUT_SCHEME']}"
      ssl_version "#{ENV['OUTPUT_SSL_VERSION']}"
      logstash_format true
      logstash_prefix "#{ENV['LOGSTASH_PREFIX']}"
      reconnect_on_error true
      <buffer>
        @type file
        #path /var/log/fluentd/fluentd-buffers/5g.core.amf.buffer
        path /home/telemetry_buffer/fluentd/fluentd-buffers/5g.core.amf.buffer
        flush_mode interval
        retry_type exponential_backoff
        flush_thread_count 2
        flush_interval 5s
        retry_forever
        retry_max_interval 30
        chunk_limit_size "#{ENV['OUTPUT_BUFFER_CHUNK_LIMIT']}"
        queue_limit_length "#{ENV['OUTPUT_BUFFER_QUEUE_LIMIT']}"
        overflow_action block
      </buffer>
    </match>

  cp.input.pcf.conf: |-
    <source>
      @id pcf_host_log
      @type tail
      path /var/log/pcf/pcf.log
      #pos_file /var/log/fluentd/pcf.log.pos
      pos_file /home/telemetry_buffer/fluentd/pcf.log.pos
      time_format %Y-%m-%dT%H:%M:%S.%N%:z # %Y-%m-%dT%H:%M:%S.%LZ
      format /^(?<message>.*)$/
      tag pcf.host.log
      read_from_head false
    </source>
    <filter pcf.host.log>      
      @type kubernetes_metadata
    </filter>                  
    #<filter pcf.host.log>
    #  @type record_transformer
    #  <record>
    #    Severity "Info"
    #    Source   "VNF"
    #    EventType "PCF"
    #  </record>
    #</filter>
    <match pcf.host.log>
      @id 5g_core_pcf_log
      @type elasticsearch
      @log_level info
      include_tag_key true
      type_name _doc
      host "#{ENV['OUTPUT_HOST']}"
      port "#{ENV['OUTPUT_PORT']}"
      scheme "#{ENV['OUTPUT_SCHEME']}"
      ssl_version "#{ENV['OUTPUT_SSL_VERSION']}"
      logstash_format true
      logstash_prefix "#{ENV['LOGSTASH_PREFIX']}"
      reconnect_on_error true
      <buffer>
        @type file
        #path /var/log/fluentd/fluentd-buffers/5g.core.pcf.buffer
        path /home/telemetry_buffer/fluentd/fluentd-buffers/5g.core.pcf.buffer
        flush_mode interval
        retry_type exponential_backoff
        flush_thread_count 2
        flush_interval 5s
        retry_forever
        retry_max_interval 30
        chunk_limit_size "#{ENV['OUTPUT_BUFFER_CHUNK_LIMIT']}"
        queue_limit_length "#{ENV['OUTPUT_BUFFER_QUEUE_LIMIT']}"
        overflow_action block
      </buffer>
    </match>

  cp.input.ausf.conf: |-
    <source>
      @id ausf_host_log
      @type tail
      path /var/log/ausf/ausf.log
      #pos_file /var/log/fluentd/ausf.log.pos
      pos_file /home/telemetry_buffer/fluentd/ausf.log.pos
      time_format %Y-%m-%dT%H:%M:%S.%N%:z # %Y-%m-%dT%H:%M:%S.%LZ
      format /^(?<message>.*)$/
      tag ausf.host.log
      read_from_head false
    </source>
    <filter ausf.host.log>      
      @type kubernetes_metadata
    </filter>                  
    #<filter ausf.host.log>
    #  @type record_transformer
    #  <record>
    #    Severity "Info"
    #    Source   "VNF"
    #    EventType "AUSF"
    #  </record>
    #</filter>
    <match ausf.host.log>
      @id 5g_core_ausf_log
      @type elasticsearch
      @log_level info
      include_tag_key true
      type_name _doc
      host "#{ENV['OUTPUT_HOST']}"
      port "#{ENV['OUTPUT_PORT']}"
      scheme "#{ENV['OUTPUT_SCHEME']}"
      ssl_version "#{ENV['OUTPUT_SSL_VERSION']}"
      logstash_format true
      logstash_prefix "#{ENV['LOGSTASH_PREFIX']}"
      reconnect_on_error true
      <buffer>
        @type file
        #path /var/log/fluentd/fluentd-buffers/5g.core.ausf.buffer
        path /home/telemetry_buffer/fluentd/fluentd-buffers/5g.core.ausf.buffer
        flush_mode interval
        retry_type exponential_backoff
        flush_thread_count 2
        flush_interval 5s
        retry_forever
        retry_max_interval 30
        chunk_limit_size "#{ENV['OUTPUT_BUFFER_CHUNK_LIMIT']}"
        queue_limit_length "#{ENV['OUTPUT_BUFFER_QUEUE_LIMIT']}"
        overflow_action block
      </buffer>
    </match>

  cp.input.udm.conf: |-
    <source>
      @id udm_host_log
      @type tail
      path /var/log/udm/udm.log
      #pos_file /var/log/fluentd/udm.log.pos
      pos_file /home/telemetry_buffer/fluentd/udm.log.pos
      time_format %Y-%m-%dT%H:%M:%S.%N%:z # %Y-%m-%dT%H:%M:%S.%LZ
      format /^(?<message>.*)$/
      tag udm.host.log
      read_from_head false
    </source>
    <filter udm.host.log>      
      @type kubernetes_metadata
    </filter>                  
    #<filter udm.host.log>
    #  @type record_transformer
    #  <record>
    #    Severity "Info"
    #    Source   "VNF"
    #    EventType "UDM"
    #  </record>
    #</filter>
    <match udm.host.log>
      @id 5g_core_udm_log
      @type elasticsearch
      @log_level info
      include_tag_key true
      type_name _doc
      host "#{ENV['OUTPUT_HOST']}"
      port "#{ENV['OUTPUT_PORT']}"
      scheme "#{ENV['OUTPUT_SCHEME']}"
      ssl_version "#{ENV['OUTPUT_SSL_VERSION']}"
      logstash_format true
      logstash_prefix "#{ENV['LOGSTASH_PREFIX']}"
      reconnect_on_error true
      <buffer>
        @type file
        #path /var/log/fluentd/fluentd-buffers/5g.core.udm.buffer
        path /home/telemetry_buffer/fluentd/fluentd-buffers/5g.core.udm.buffer
        flush_mode interval
        retry_type exponential_backoff
        flush_thread_count 2
        flush_interval 5s
        retry_forever
        retry_max_interval 30
        chunk_limit_size "#{ENV['OUTPUT_BUFFER_CHUNK_LIMIT']}"
        queue_limit_length "#{ENV['OUTPUT_BUFFER_QUEUE_LIMIT']}"
        overflow_action block
      </buffer>
    </match>

  system.input.conf: |-
    # Logs from systemd-journal for interesting services.
    #<source>
    #  @id journald-docker
    #  @type systemd
    #  matches [{ "_SYSTEMD_UNIT": "docker.service" }]
    #  <storage>
    #    @type local
    #    persistent true
    #    path /var/log/fluentd/journald-docker.pos
    #  </storage>
    #  read_from_head true
    #  tag docker
    #</source>
    <source>                                         
      @id journald-crio                            
      @type systemd                                  
      matches [{ "_SYSTEMD_UNIT": "crio.service" }]
      <storage>                                      
        @type local                                  
        persistent true                              
        #path /var/log/fluentd/journald-crio.pos    
        path /home/telemetry_buffer/fluentd/journald-crio.pos
      </storage>                                     
      read_from_head false #true                            
      tag crio                                     
    </source>                                        
    <source>
      @id journald-kubelet
      @type systemd
      matches [{ "_SYSTEMD_UNIT": "kubelet.service" }]
      <storage>
        @type local
        persistent true
        #path /var/log/fluentd/journald-kubelet.pos
        path /home/telemetry_buffer/fluentd/journald-kubelet.pos
      </storage>
      read_from_head false #true
      tag kubelet
    </source>
    #<filter crio kubelet>
    #  @type record_transformer
    #  <record>
    #    Severity "Info"
    #    Source   "Host"
    #    EventType "Systemd"
    #  </record>
    #</filter>
    #<source>
    #  @id journald-node-problem-detector
    #  @type systemd
    #  matches [{ "_SYSTEMD_UNIT": "node-problem-detector.service" }]
    #  <storage>
    #    @type local
    #    persistent true
    #    path /var/log/fluentd/journald-node-problem-detector.pos
    #  </storage>
    #  read_from_head true
    #  tag node-problem-detector
    #</source>
    <match kubelet crio docker >                
      @id host.system.log                                       
      @type elasticsearch                                       
      @log_level info                                           
      include_tag_key true                                      
      type_name _doc                                            
      host "#{ENV['OUTPUT_HOST']}"                              
      port "#{ENV['OUTPUT_PORT']}"                              
      scheme "#{ENV['OUTPUT_SCHEME']}"                          
      ssl_version "#{ENV['OUTPUT_SSL_VERSION']}"                
      logstash_format true                                      
      logstash_prefix "host-system"                             
      reconnect_on_error true                                   
      <buffer>                                                  
        @type file                                              
        #path /var/log/fluentd/fluentd-buffers/host.system.buffer
        path /home/telemetry_buffer/fluentd/fluentd-buffers/host.system.buffer
        flush_mode interval                                     
        retry_type exponential_backoff                          
        flush_thread_count 2                                    
        flush_interval 5s                                       
        retry_forever                                           
        retry_max_interval 30                                   
        chunk_limit_size "#{ENV['OUTPUT_BUFFER_CHUNK_LIMIT']}"  
        queue_limit_length "#{ENV['OUTPUT_BUFFER_QUEUE_LIMIT']}"
        overflow_action block                                   
      </buffer>                                                 
    </match>                                                    

  containers.input.conf: |-
    <source>
      @id fluentd-containers.log
      @type tail
      path /var/log/containers/*.log
      #pos_file /var/log/fluentd/fluentd-containers.pos
      pos_file /home/telemetry_buffer/fluentd/fluentd-containers.pos
      #<storage>                                   
      #  @type local                               
      #  persistent true                           
      #  path /var/log/fluentd/fluentd-containers.pos
      #</storage>                                  
      time_format %Y-%m-%dT%H:%M:%S.%NZ
      tag raw.kubernetes.*
      #format json
      read_from_head false # true
      #Ben
      <parse>
        @type multi_format
        <pattern>
          format json
          time_format %Y-%m-%dT%H:%M:%S.%NZ
        </pattern>
        <pattern>
          format /^(?<time>.+) (?<stream>stdout|stderr) [^ ]* (?<log>.*)$/
          time_format %Y-%m-%dT%H:%M:%S.%N%:z
        </pattern>
      </parse>
    </source>
    #<filter raw.kubernetes.**>
    #  @type record_transformer
    #  <record>
    #    Severity "Info"
    #    Source   "Openshift"
    #    EventType "Containers"
    #  </record>
    #</filter>
    # Detect exceptions in the log output and forward them as one log entry.
    <match raw.kubernetes.**>
      @id raw.kubernetes
      @type detect_exceptions
      remove_tag_prefix raw
      message log
      stream stream
      multiline_flush_interval 5
      max_bytes 500000
      max_lines 1000
    </match>
    <filter kubernetes.**>
      @type kubernetes_metadata
    </filter>
    <match kubernetes.**>
      @id elasticsearch_pod
      @type elasticsearch
      @log_level info
      include_tag_key true
      #type_name _doc
      host "#{ENV['OUTPUT_HOST']}"
      port "#{ENV['OUTPUT_PORT']}"
      scheme "#{ENV['OUTPUT_SCHEME']}"
      ssl_version "#{ENV['OUTPUT_SSL_VERSION']}"
      logstash_format true
      logstash_prefix "containers"
      #logstash_prefix "#{ENV['LOGSTASH_PREFIX']}"
      reconnect_on_error true
      <buffer>
        @type file
        #path /var/log/fluentd/fluentd-buffers/kubernetes.system.pod.buffer
        path /home/telemetry_buffer/fluentd/fluentd-buffers/kubernetes.system.pod.buffer
        flush_mode interval
        retry_type exponential_backoff
        flush_thread_count 2
        flush_interval 5s
        retry_forever
        retry_max_interval 30
        chunk_limit_size "#{ENV['OUTPUT_BUFFER_CHUNK_LIMIT']}"
        queue_limit_length "#{ENV['OUTPUT_BUFFER_QUEUE_LIMIT']}"
        overflow_action block
      </buffer>
    </match>


  forward.input.conf: |-
    # Takes the messages sent over TCP
    <source>
      @id forward
      @type forward
    </source>
  #monitoring.conf: |-
  #  # Prometheus Exporter Plugin
  #  # input plugin that exports metrics
  #  <source>
  #    @type prometheus
  #  </source>
  #  <source>
  #    @type monitor_agent
  #  </source>
  #  # input plugin that collects metrics from MonitorAgent
  #  <source>
  #    @type prometheus_monitor
  #    <labels>
  #      host ${hostname}
  #    </labels>
  #  </source>
  #  # input plugin that collects metrics for output plugin
  #  <source>
  #    @type prometheus_output_monitor
  #    <labels>
  #      host ${hostname}
  #    </labels>
  #  </source>
  #  # input plugin that collects metrics for in_tail plugin
  #  <source>
  #    @type prometheus_tail_monitor
  #    <labels>
  #      host ${hostname}
  #    </labels>
  #  </source>
  ##output.conf: |
  ##  # Enriches records with Kubernetes metadata
  ##  <filter kubernetes.**>
  ##    @type kubernetes_metadata
  ##  </filter>

  ##  #<filter **.amf-*.log **.smf-*.log **.ausf-*.log **.pcf-*.log **.udm-*.log **.upf-*.log upf.host.log amf.host.log smf.host.log>
  ##  #  @type record_transformer
  ##  #  enable_ruby
  ##  #  <record>
  ##  #    uid ${DateTime.now.strftime('%s%6N')[-7..-1]}
  ##  #  </record>
  ##  #</filter>

  ##  #<match **.amf-*.log **.smf-*.log **.ausf-*.log **.pcf-*.log **.udm-*.log **.upf-*.log upf.host.log amf.host.log smf.host.log>
  ##  #  @id 5g.core.log
  ##  #  @type elasticsearch
  ##  #  @log_level info
  ##  #  include_tag_key true
  ##  #  type_name _doc
  ##  #  host "#{ENV['OUTPUT_HOST']}"
  ##  #  port "#{ENV['OUTPUT_PORT']}"
  ##  #  scheme "#{ENV['OUTPUT_SCHEME']}"
  ##  #  ssl_version "#{ENV['OUTPUT_SSL_VERSION']}"
  ##  #  logstash_format true
  ##  #  logstash_prefix "#{ENV['LOGSTASH_PREFIX']}"
  ##  #  reconnect_on_error true
  ##  #  <buffer>
  ##  #    @type file
  ##  #    path /var/log/fluentd/fluentd-buffers/5g.core.buffer
  ##  #    flush_mode interval
  ##  #    retry_type exponential_backoff
  ##  #    flush_thread_count 2
  ##  #    flush_interval 5s
  ##  #    retry_forever
  ##  #    retry_max_interval 30
  ##  #    chunk_limit_size "#{ENV['OUTPUT_BUFFER_CHUNK_LIMIT']}"
  ##  #    queue_limit_length "#{ENV['OUTPUT_BUFFER_QUEUE_LIMIT']}"
  ##  #    overflow_action block
  ##  #  </buffer>
  ##  #</match>

  ##  <match **.log>
  ##    @id 5g.system.log
  ##    @type elasticsearch
  ##    @log_level info
  ##    include_tag_key true
  ##    type_name _doc
  ##    host "#{ENV['OUTPUT_HOST']}"
  ##    port "#{ENV['OUTPUT_PORT']}"
  ##    scheme "#{ENV['OUTPUT_SCHEME']}"
  ##    ssl_version "#{ENV['OUTPUT_SSL_VERSION']}"
  ##    logstash_format true
  ##    logstash_prefix "5g-support"
  ##    reconnect_on_error true
  ##    <buffer>
  ##      @type file
  ##      path /var/log/fluentd/fluentd-buffers/5g.system.buffer
  ##      flush_mode interval
  ##      retry_type exponential_backoff
  ##      flush_thread_count 2
  ##      flush_interval 5s
  ##      retry_forever
  ##      retry_max_interval 30
  ##      chunk_limit_size "#{ENV['OUTPUT_BUFFER_CHUNK_LIMIT']}"
  ##      queue_limit_length "#{ENV['OUTPUT_BUFFER_QUEUE_LIMIT']}"
  ##      overflow_action block
  ##    </buffer>
  ##  </match>

  ##  <match kubelet docker node-problem-detector>
  ##    @id host.system.log
  ##    @type elasticsearch
  ##    @log_level info
  ##    include_tag_key true
  ##    type_name _doc
  ##    host "#{ENV['OUTPUT_HOST']}"
  ##    port "#{ENV['OUTPUT_PORT']}"
  ##    scheme "#{ENV['OUTPUT_SCHEME']}"
  ##    ssl_version "#{ENV['OUTPUT_SSL_VERSION']}"
  ##    logstash_format true
  ##    logstash_prefix "host-system"
  ##    reconnect_on_error true
  ##    <buffer>
  ##      @type file
  ##      path /var/log/fluentd/fluentd-buffers/host.system.buffer
  ##      flush_mode interval
  ##      retry_type exponential_backoff
  ##      flush_thread_count 2
  ##      flush_interval 5s
  ##      retry_forever
  ##      retry_max_interval 30
  ##      chunk_limit_size "#{ENV['OUTPUT_BUFFER_CHUNK_LIMIT']}"
  ##      queue_limit_length "#{ENV['OUTPUT_BUFFER_QUEUE_LIMIT']}"
  ##      overflow_action block
  ##    </buffer>
  ##  </match>

# extraVolumes:
#   - name: es-certs
#     secret:
#       defaultMode: 420
#       secretName: es-certs
# extraVolumeMounts:
#   - name: es-certs
#     mountPath: /certs
#     readOnly: true
