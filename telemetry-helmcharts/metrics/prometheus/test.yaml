apiVersion: v1
data:
  openshift-cloud-credential-operator-cloud-credential-operator-alerts.yaml: |
    groups:
    - name: CloudCredentialOperator
      rules:
      - alert: CloudCredentialOperatorTargetNamespaceMissing
        annotations:
          summary: CredentialsRequest(s) pointing to non-existant namespace
        expr: cco_credentials_requests_conditions{condition="MissingTargetNamespace"}
          > 0
        for: 5m
        labels:
          severity: warning
      - alert: CloudCredentialOperatorProvisioningFailed
        annotations:
          summary: CredentialsRequest(s) unable to be fulfilled
        expr: cco_credentials_requests_conditions{condition="CredentialsProvisionFailure"}
          > 0
        for: 5m
        labels:
          severity: warning
      - alert: CloudCredentialOperatorDeprovisioningFailed
        annotations:
          summary: CredentialsRequest(s) unable to be cleaned up
        expr: cco_credentials_requests_conditions{condition="CredentialsDeprovisionFailure"}
          > 0
        for: 5m
        labels:
          severity: warning
      - alert: CloudCredentialOperatorInsufficientCloudCreds
        annotations:
          summary: Cluster's cloud credentials insufficient for minting or passthrough
        expr: cco_credentials_requests_conditions{condition="InsufficientCloudCreds"}
          > 0
        for: 5m
        labels:
          severity: warning
      - alert: CloudCredentialOperatorDown
        annotations:
          summary: cloud-credential-operator pod not running
        expr: absent(up{job="cco-metrics"} == 1)
        for: 5m
        labels:
          severity: critical
  openshift-cluster-machine-approver-machineapprover-rules.yaml: |
    groups:
    - name: general.rules
      rules:
      - alert: ClusterMachineApproverDown
        annotations:
          message: ClusterMachineApprover has disappeared from Prometheus target discovery.
        expr: |
          absent(up{job="machine-approver"} == 1)
        for: 10m
        labels:
          severity: critical
      - alert: MachineApproverMaxPendingCSRsReached
        annotations:
          message: max pending CSRs threshold reached.
        expr: |
          mapi_current_pending_csr > mapi_max_pending_csr
        for: 5m
        labels:
          severity: warning
  openshift-cluster-samples-operator-samples-operator-alerts.yaml: |
    groups:
    - name: SamplesOperator
      rules:
      - alert: SamplesDegraded
        annotations:
          message: |
            Samples could not be deployed and are in Degraded status. You can look at the "openshift-samples" ClusterOperator object for details. You can also query 'openshift_samples_failed_imagestream_import_info' to see if ImageStreams do not import. The samples operator reports itself Degraded if an ImageStream import continues to fail for 2 hours.
        expr: openshift_samples_degraded_info == 1
        for: 2h
        labels:
          severity: warning
      - alert: SamplesInvalidConfig
        annotations:
          message: |
            Samples operator has been given an invalid configuration.
        expr: openshift_samples_invalidconfig_info == 1
        for: 2h
        labels:
          severity: warning
      - alert: SamplesMissingSecret
        annotations:
          message: |
            Samples operator cannot find the samples pull secret in the openshift namespace.
        expr: openshift_samples_invalidsecret_info{reason="missing_secret"} == 1
        for: 2h
        labels:
          severity: warning
      - alert: SamplesMissingTBRCredential
        annotations:
          message: |
            Samples operator cannot find credentials for 'registry.redhat.io' in the samples pull secret in the openshift namespace.  Unless you have updated the 'samplesRegistry' field in the samples operator config to change which registry the sample ImageStreams are pulled from, you will have failed imports for many of the sample ImageStreams in this case.
        expr: openshift_samples_invalidsecret_info{reason="missing_tbr_credential"} ==
          1
        for: 2h
        labels:
          severity: warning
  openshift-cluster-version-cluster-version-operator.yaml: |
    groups:
    - name: cluster-version
      rules:
      - alert: ClusterVersionOperatorDown
        annotations:
          message: Cluster version operator has disappeared from Prometheus target discovery.
            Operator may be down or disabled, cluster will not be kept up to date and
            upgrades will not be possible.
        expr: |
          absent(up{job="cluster-version-operator"} == 1)
        for: 10m
        labels:
          severity: critical
    - name: cluster-operators
      rules:
      - alert: ClusterOperatorDown
        annotations:
          message: Cluster operator {{ $labels.name }} has not been available for 10 mins.
            Operator may be down or disabled, cluster will not be kept up to date and
            upgrades will not be possible.
        expr: |
          cluster_operator_up{job="cluster-version-operator"} == 0
        for: 10m
        labels:
          severity: critical
      - alert: ClusterOperatorDegraded
        annotations:
          message: Cluster operator {{ $labels.name }} has been degraded for 10 mins.
            Operator is degraded because {{ $labels.reason }} and cluster upgrades will
            be unstable.
        expr: |
          cluster_operator_conditions{job="cluster-version-operator", condition="Degraded"} == 1
        for: 10m
        labels:
          severity: critical
      - alert: ClusterOperatorFlapping
        annotations:
          message: Cluster operator {{ $labels.name }} up status is changing often. This
            might cause upgrades to be unstable.
        expr: |
          changes(cluster_operator_up{job="cluster-version-operator"}[2m]) > 2
        for: 10m
        labels:
          severity: warning
  openshift-image-registry-image-registry-operator-alerts.yaml: |
    groups:
    - name: ImageRegistryOperator
      rules:
      - alert: ImageRegistryStorageReconfigured
        annotations:
          message: |
            Image Registry Storage configuration has changed in the last 30
            minutes. This change may have caused data loss.
        expr: increase(image_registry_operator_storage_reconfigured_total[30m]) > 0
        labels:
          severity: warning
      - alert: ImageRegistryRemoved
        annotations:
          message: |
            Image Registry has been removed. ImageStreamTags, BuildConfigs and
            DeploymentConfigs which reference ImageStreamTags may not work as
            expected. Please configure storage and update the config to Managed
            state by editing configs.imageregistry.operator.openshift.io.
        expr: cluster_operator_conditions{name="image-registry",condition="Available",reason="Removed"}
          > 0
        for: 5m
        labels:
          severity: warning
  openshift-kube-apiserver-kube-apiserver.yaml: |
    groups:
    - name: using-deprecated-apis
      rules:
      - alert: UsingDeprecatedAPIAppsV1Beta1
        annotations:
          message: A client in the cluster is using deprecated apps/v1beta1 API that will
            be removed soon.
        expr: |
          apiserver_request_count{group="apps",version="v1beta1"}
        labels:
          severity: warning
      - alert: UsingDeprecatedAPIAppsV1Beta2
        annotations:
          message: A client in the cluster is using deprecated apps/v1beta2 API that will
            be removed soon.
        expr: |
          apiserver_request_count{group="apps",version="v1beta2"}
        labels:
          severity: warning
      - alert: UsingDeprecatedAPIExtensionsV1Beta1
        annotations:
          message: A client in the cluster is using deprecated extensions/v1beta1 API
            that will be removed soon.
        expr: |
          apiserver_request_count{group="extensions",version="v1beta1",resource!~"ingresses|",client!~"hyperkube/.*|cluster-policy-controller/.*"}
        labels:
          severity: warning
  openshift-kube-apiserver-operator-kube-apiserver-operator.yaml: |
    groups:
    - name: cluster-version
      rules:
      - alert: TechPreviewNoUpgrade
        annotations:
          message: Cluster has enabled tech preview features that will prevent upgrades.
        expr: |
          cluster_feature_set{name!="", namespace="openshift-kube-apiserver-operator"} == 0
        for: 10m
        labels:
          severity: warning
  openshift-kube-controller-manager-operator-kube-controller-manager-operator.yaml: |
    groups:
    - name: cluster-version
      rules:
      - alert: PodDisruptionBudgetAtLimit
        annotations:
          message: The pod disruption budget is preventing further disruption to pods
            because it is at the minimum allowed level.
        expr: |
          kube_poddisruptionbudget_status_expected_pods == on(namespace, poddisruptionbudget, service) kube_poddisruptionbudget_status_desired_healthy
        for: 15m
        labels:
          severity: warning
      - alert: PodDisruptionBudgetLimit
        annotations:
          message: The pod disruption budget is below the minimum number allowed pods.
        expr: |
          kube_poddisruptionbudget_status_expected_pods < on(namespace, poddisruptionbudget, service) kube_poddisruptionbudget_status_desired_healthy
        for: 15m
        labels:
          severity: critical
  openshift-kube-scheduler-operator-kube-scheduler-operator.yaml: |
    groups:
    - name: cluster-version
      rules:
      - alert: KubeSchedulerDown
        annotations:
          message: KubeScheduler has disappeared from Prometheus target discovery.
        expr: |
          absent(up{job="scheduler"} == 1)
        for: 15m
        labels:
          severity: critical
  openshift-machine-api-cluster-autoscaler-operator-rules.yaml: |
    groups:
    - name: Cluster-autoscaler-operator-down
      rules:
      - alert: ClusterAutoscalerOperatorDown
        annotations:
          message: cluster-autoscaler-operator has disappeared from Prometheus target
            discovery.
        expr: |
          absent(up{job="cluster-autoscaler-operator"} == 1)
        for: 5m
        labels:
          severity: critical
  openshift-machine-api-machine-api-operator-prometheus-rules.yaml: |
    groups:
    - name: machine-without-valid-node-ref
      rules:
      - alert: MachineWithoutValidNode
        annotations:
          message: machine {{ $labels.name }} does not have valid node reference
        expr: |
          (mapi_machine_created_timestamp_seconds unless on(node) kube_node_info) > 0
        for: 10m
        labels:
          severity: critical
    - name: machine-with-no-running-phase
      rules:
      - alert: MachineWithNoRunningPhase
        annotations:
          message: machine {{ $labels.name }} is in {{ $labels.phase }} phase
        expr: |
          (mapi_machine_created_timestamp_seconds{phase!="Running"}) > 0
        for: 10m
        labels:
          severity: critical
    - name: machine-api-operator-metrics-collector-up
      rules:
      - alert: MachineAPIOperatorMetricsCollectionFailing
        annotations:
          message: 'machine api operator metrics collection is failing. For more details:  oc
            logs <machine-api-operator-pod-name> -n openshift-machine-api'
        expr: |
          mapi_mao_collector_up == 0
        for: 3m
        labels:
          severity: critical
    - name: machine-api-operator-down
      rules:
      - alert: MachineAPIOperatorDown
        annotations:
          message: machine api operator is down
        expr: |
          absent(up{job="machine-api-operator"} == 1)
        for: 3m
        labels:
          severity: critical
  openshift-machine-config-operator-machine-config-daemon.yaml: |
    groups:
    - name: mcd-reboot-error
      rules:
      - alert: MCDRebootError
        annotations:
          message: Reboot failed on {{ $labels.node }} , update may be blocked
        expr: |
          mcd_reboot_err > 0
        labels:
          severity: critical
    - name: mcd-drain-error
      rules:
      - alert: MCDDrainError
        annotations:
          message: 'Drain failed on {{ $labels.node }} , updates may be blocked. For more
            details:  oc logs -f -n openshift-machine-config-operator machine-config-daemon-<hash>
            -c machine-config-daemon'
        expr: |
          mcd_drain > 0
        labels:
          severity: critical
    - name: mcd-pivot-error
      rules:
      - alert: MCDPivotError
        annotations:
          message: 'Error detected in pivot logs on {{ $labels.node }} '
        expr: |
          mcd_pivot_err > 0
        labels:
          severity: warning
    - name: mcd-kubelet-health-state-error
      rules:
      - alert: KubeletHealthState
        annotations:
          message: Kubelet health failure threshold reached
        expr: |
          mcd_kubelet_state > 2
        labels:
          severity: warning
  openshift-marketplace-marketplace-alert-rules.yaml: |
    groups:
    - name: marketplace.community_operators.rules
      rules:
      - expr: sum(app_registry_request_total{code=~"1..",opsrc="community-operators"})
        record: app_registry:community_operators:1xx_response
      - expr: sum(app_registry_request_total{code=~"2..",opsrc="community-operators"})
        record: app_registry:community_operators:2xx_response
      - expr: sum(app_registry_request_total{code=~"3..",opsrc="community-operators"})
        record: app_registry:community_operators:3xx_response
      - expr: sum(app_registry_request_total{code=~"4..",opsrc="community-operators"})
        record: app_registry:community_operators:4xx_response
      - expr: sum(app_registry_request_total{code=~"5..",opsrc="community-operators"})
        record: app_registry:community_operators:5xx_response
      - alert: CommunityOperatorConnectionErrors
        annotations:
          message: Unable to connect to the default OperatorSource community-operators
            AppRegistry for {{ $value }}% of requests.
        expr: rate(app_registry_request_total{code!~"2..",opsrc="community-operators"}[5m])
          / rate(app_registry_request_total{opsrc="community-operators"}[5m]) >= 0.5
        for: 15m
        labels:
          severity: warning
    - name: marketplace.certified_operators.rules
      rules:
      - expr: sum(app_registry_request_total{code=~"1..",opsrc="certified-operators"})
        record: app_registry:certified_operators:1xx_response
      - expr: sum(app_registry_request_total{code=~"2..",opsrc="certified-operators"})
        record: app_registry:certified_operators:2xx_response
      - expr: sum(app_registry_request_total{code=~"3..",opsrc="certified-operators"})
        record: app_registry:certified_operators:3xx_response
      - expr: sum(app_registry_request_total{code=~"4..",opsrc="certified-operators"})
        record: app_registry:certified_operators:4xx_response
      - expr: sum(app_registry_request_total{code=~"5..",opsrc="certified-operators"})
        record: app_registry:certified_operators:5xx_response
      - alert: CertifiedOperatorConnectionErrors
        annotations:
          message: Unable to connect to the default OperatorSource certified-operators
            AppRegistry for {{ $value }}% of requests.
        expr: rate(app_registry_request_total{code!~"2..",opsrc="certified-operators"}[5m])
          / rate(app_registry_request_total{opsrc="certified-operators"}[5m]) >= 0.5
        for: 15m
        labels:
          severity: warning
    - name: marketplace.redhat_operators.rules
      rules:
      - expr: sum(app_registry_request_total{code=~"1..",opsrc="redhat-operators"})
        record: app_registry:redhat_operators:1xx_response
      - expr: sum(app_registry_request_total{code=~"2..",opsrc="redhat-operators"})
        record: app_registry:redhat_operators:2xx_response
      - expr: sum(app_registry_request_total{code=~"3..",opsrc="redhat-operators"})
        record: app_registry:redhat_operators:3xx_response
      - expr: sum(app_registry_request_total{code=~"4..",opsrc="redhat-operators"})
        record: app_registry:redhat_operators:4xx_response
      - expr: sum(app_registry_request_total{code=~"5..",opsrc="redhat-operators"})
        record: app_registry:redhat_operators:5xx_response
      - alert: RedhatOperatorConnectionErrors
        annotations:
          message: Unable to connect to the default OperatorSource redhat-operators AppRegistry
            for {{ $value }}% of requests.
        expr: rate(app_registry_request_total{code!~"2..",opsrc="redhat-operators"}[5m])
          / rate(app_registry_request_total{opsrc="redhat-operators"}[5m]) >= 0.5
        for: 15m
        labels:
          severity: warning
  openshift-monitoring-prometheus-k8s-rules.yaml: |
    groups:
    - name: node-exporter.rules
      rules:
      - expr: |
          count without (cpu) (
            count without (mode) (
              node_cpu_seconds_total{job="node-exporter"}
            )
          )
        record: instance:node_num_cpu:sum
      - expr: |
          1 - avg without (cpu, mode) (
            rate(node_cpu_seconds_total{job="node-exporter", mode="idle"}[1m])
          )
        record: instance:node_cpu_utilisation:rate1m
      - expr: |
          (
            node_load1{job="node-exporter"}
          /
            instance:node_num_cpu:sum{job="node-exporter"}
          )
        record: instance:node_load1_per_cpu:ratio
      - expr: |
          1 - (
            node_memory_MemAvailable_bytes{job="node-exporter"}
          /
            node_memory_MemTotal_bytes{job="node-exporter"}
          )
        record: instance:node_memory_utilisation:ratio
      - expr: |
          rate(node_vmstat_pgmajfault{job="node-exporter"}[1m])
        record: instance:node_vmstat_pgmajfault:rate1m
      - expr: |
          rate(node_disk_io_time_seconds_total{job="node-exporter", device=~"nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+"}[1m])
        record: instance_device:node_disk_io_time_seconds:rate1m
      - expr: |
          rate(node_disk_io_time_weighted_seconds_total{job="node-exporter", device=~"nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+"}[1m])
        record: instance_device:node_disk_io_time_weighted_seconds:rate1m
      - expr: |
          sum without (device) (
            rate(node_network_receive_bytes_total{job="node-exporter", device!="lo"}[1m])
          )
        record: instance:node_network_receive_bytes_excluding_lo:rate1m
      - expr: |
          sum without (device) (
            rate(node_network_transmit_bytes_total{job="node-exporter", device!="lo"}[1m])
          )
        record: instance:node_network_transmit_bytes_excluding_lo:rate1m
      - expr: |
          sum without (device) (
            rate(node_network_receive_drop_total{job="node-exporter", device!="lo"}[1m])
          )
        record: instance:node_network_receive_drop_excluding_lo:rate1m
      - expr: |
          sum without (device) (
            rate(node_network_transmit_drop_total{job="node-exporter", device!="lo"}[1m])
          )
        record: instance:node_network_transmit_drop_excluding_lo:rate1m
    - name: kube-apiserver.rules
      rules:
      - expr: |
          sum(rate(apiserver_request_duration_seconds_sum{subresource!="log",verb!~"LIST|WATCH|WATCHLIST|PROXY|CONNECT"}[5m])) without(instance, pod)
          /
          sum(rate(apiserver_request_duration_seconds_count{subresource!="log",verb!~"LIST|WATCH|WATCHLIST|PROXY|CONNECT"}[5m])) without(instance, pod)
        record: cluster:apiserver_request_duration_seconds:mean5m
      - expr: |
          histogram_quantile(0.99, sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",subresource!="log",verb!~"LIST|WATCH|WATCHLIST|PROXY|CONNECT"}[5m])) without(instance, pod))
        labels:
          quantile: "0.99"
        record: cluster_quantile:apiserver_request_duration_seconds:histogram_quantile
      - expr: |
          histogram_quantile(0.9, sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",subresource!="log",verb!~"LIST|WATCH|WATCHLIST|PROXY|CONNECT"}[5m])) without(instance, pod))
        labels:
          quantile: "0.9"
        record: cluster_quantile:apiserver_request_duration_seconds:histogram_quantile
      - expr: |
          histogram_quantile(0.5, sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",subresource!="log",verb!~"LIST|WATCH|WATCHLIST|PROXY|CONNECT"}[5m])) without(instance, pod))
        labels:
          quantile: "0.5"
        record: cluster_quantile:apiserver_request_duration_seconds:histogram_quantile
    - name: k8s.rules
      rules:
      - expr: |
          sum(rate(container_cpu_usage_seconds_total{job="kubelet", image!="", container!="POD"}[5m])) by (namespace)
        record: namespace:container_cpu_usage_seconds_total:sum_rate
      - expr: |
          sum by (namespace, pod, container) (
            rate(container_cpu_usage_seconds_total{job="kubelet", image!="", container!="POD"}[5m])
          ) * on (namespace, pod) group_left(node) max by(namespace, pod, node) (kube_pod_info)
        record: node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate
      - expr: |
          container_memory_working_set_bytes{job="kubelet", image!=""}
          * on (namespace, pod) group_left(node) max by(namespace, pod, node) (kube_pod_info)
        record: node_namespace_pod_container:container_memory_working_set_bytes
      - expr: |
          container_memory_rss{job="kubelet", image!=""}
          * on (namespace, pod) group_left(node) max by(namespace, pod, node) (kube_pod_info)
        record: node_namespace_pod_container:container_memory_rss
      - expr: |
          container_memory_cache{job="kubelet", image!=""}
          * on (namespace, pod) group_left(node) max by(namespace, pod, node) (kube_pod_info)
        record: node_namespace_pod_container:container_memory_cache
      - expr: |
          container_memory_swap{job="kubelet", image!=""}
          * on (namespace, pod) group_left(node) max by(namespace, pod, node) (kube_pod_info)
        record: node_namespace_pod_container:container_memory_swap
      - expr: |
          sum(container_memory_usage_bytes{job="kubelet", image!="", container!="POD"}) by (namespace)
        record: namespace:container_memory_usage_bytes:sum
      - expr: |
          sum by (namespace, label_name) (
              sum(kube_pod_container_resource_requests_memory_bytes{job="kube-state-metrics"} * on (endpoint, instance, job, namespace, pod, service) group_left(phase) (kube_pod_status_phase{phase=~"Pending|Running"} == 1)) by (namespace, pod)
            * on (namespace, pod)
              group_left(label_name) kube_pod_labels{job="kube-state-metrics"}
          )
        record: namespace:kube_pod_container_resource_requests_memory_bytes:sum
      - expr: |
          sum by (namespace, label_name) (
              sum(kube_pod_container_resource_requests_cpu_cores{job="kube-state-metrics"} * on (endpoint, instance, job, namespace, pod, service) group_left(phase) (kube_pod_status_phase{phase=~"Pending|Running"} == 1)) by (namespace, pod)
            * on (namespace, pod)
              group_left(label_name) kube_pod_labels{job="kube-state-metrics"}
          )
        record: namespace:kube_pod_container_resource_requests_cpu_cores:sum
      - expr: |
          sum(
            label_replace(
              label_replace(
                kube_pod_owner{job="kube-state-metrics", owner_kind="ReplicaSet"},
                "replicaset", "$1", "owner_name", "(.*)"
              ) * on(replicaset, namespace) group_left(owner_name) kube_replicaset_owner{job="kube-state-metrics"},
              "workload", "$1", "owner_name", "(.*)"
            )
          ) by (namespace, workload, pod)
        labels:
          workload_type: deployment
        record: mixin_pod_workload
      - expr: |
          sum(
            label_replace(
              kube_pod_owner{job="kube-state-metrics", owner_kind="DaemonSet"},
              "workload", "$1", "owner_name", "(.*)"
            )
          ) by (namespace, workload, pod)
        labels:
          workload_type: daemonset
        record: mixin_pod_workload
      - expr: |
          sum(
            label_replace(
              kube_pod_owner{job="kube-state-metrics", owner_kind="StatefulSet"},
              "workload", "$1", "owner_name", "(.*)"
            )
          ) by (namespace, workload, pod)
        labels:
          workload_type: statefulset
        record: mixin_pod_workload
    - name: kube-scheduler.rules
      rules:
      - expr: |
          histogram_quantile(0.99, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: "0.99"
        record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
      - expr: |
          histogram_quantile(0.99, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: "0.99"
        record: cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
      - expr: |
          histogram_quantile(0.99, sum(rate(scheduler_binding_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: "0.99"
        record: cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
      - expr: |
          histogram_quantile(0.9, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: "0.9"
        record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
      - expr: |
          histogram_quantile(0.9, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: "0.9"
        record: cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
      - expr: |
          histogram_quantile(0.9, sum(rate(scheduler_binding_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: "0.9"
        record: cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
      - expr: |
          histogram_quantile(0.5, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: "0.5"
        record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
      - expr: |
          histogram_quantile(0.5, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: "0.5"
        record: cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
      - expr: |
          histogram_quantile(0.5, sum(rate(scheduler_binding_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: "0.5"
        record: cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
    - name: node.rules
      rules:
      - expr: sum(min(kube_pod_info) by (node))
        record: ':kube_pod_info_node_count:'
      - expr: |
          max(label_replace(kube_pod_info{job="kube-state-metrics"}, "pod", "$1", "pod", "(.*)")) by (node, namespace, pod)
        record: 'node_namespace_pod:kube_pod_info:'
      - expr: |
          count by (node) (sum by (node, cpu) (
            node_cpu_seconds_total{job="node-exporter"}
          * on (namespace, pod) group_left(node)
            node_namespace_pod:kube_pod_info:
          ))
        record: node:node_num_cpu:sum
      - expr: |
          sum(
            node_memory_MemAvailable_bytes{job="node-exporter"} or
            (
              node_memory_Buffers_bytes{job="node-exporter"} +
              node_memory_Cached_bytes{job="node-exporter"} +
              node_memory_MemFree_bytes{job="node-exporter"} +
              node_memory_Slab_bytes{job="node-exporter"}
            )
          )
        record: :node_memory_MemAvailable_bytes:sum
    - name: kube-prometheus-node-recording.rules
      rules:
      - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait"}[3m])) BY (instance)
        record: instance:node_cpu:rate:sum
      - expr: sum((node_filesystem_size_bytes{mountpoint="/"} - node_filesystem_free_bytes{mountpoint="/"}))
          BY (instance)
        record: instance:node_filesystem_usage:sum
      - expr: sum(rate(node_network_receive_bytes_total[3m])) BY (instance)
        record: instance:node_network_receive_bytes:rate:sum
      - expr: sum(rate(node_network_transmit_bytes_total[3m])) BY (instance)
        record: instance:node_network_transmit_bytes:rate:sum
      - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait"}[5m])) WITHOUT
          (cpu, mode) / ON(instance) GROUP_LEFT() count(sum(node_cpu_seconds_total) BY
          (instance, cpu)) BY (instance)
        record: instance:node_cpu:ratio
      - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait"}[5m]))
        record: cluster:node_cpu:sum_rate5m
      - expr: cluster:node_cpu_seconds_total:rate5m / count(sum(node_cpu_seconds_total)
          BY (instance, cpu))
        record: cluster:node_cpu:ratio
    - name: kubernetes.rules
      rules:
      - expr: sum(container_memory_usage_bytes{container="",pod!=""}) BY (pod, namespace)
        record: pod:container_memory_usage_bytes:sum
      - expr: sum(container_spec_cpu_shares{container="",pod!=""}) BY (pod, namespace)
        record: pod:container_spec_cpu_shares:sum
      - expr: sum(rate(container_cpu_usage_seconds_total{container="",pod!=""}[5m])) BY
          (pod, namespace)
        record: pod:container_cpu_usage:sum
      - expr: sum(container_fs_usage_bytes{pod!=""}) BY (pod, namespace)
        record: pod:container_fs_usage_bytes:sum
      - expr: sum(container_memory_usage_bytes{container!=""}) BY (namespace)
        record: namespace:container_memory_usage_bytes:sum
      - expr: sum(container_spec_cpu_shares{container!=""}) BY (namespace)
        record: namespace:container_spec_cpu_shares:sum
      - expr: sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[5m]))
          BY (namespace)
        record: namespace:container_cpu_usage:sum
      - expr: sum(container_memory_usage_bytes{container="",pod!=""}) BY (cluster) / sum(machine_memory_bytes)
          BY (cluster)
        record: cluster:memory_usage:ratio
      - expr: sum(container_spec_cpu_shares{container="",pod!=""}) / 1000 / sum(machine_cpu_cores)
        record: cluster:container_spec_cpu_shares:ratio
      - expr: sum(rate(container_cpu_usage_seconds_total{container="",pod!=""}[5m])) /
          sum(machine_cpu_cores)
        record: cluster:container_cpu_usage:ratio
      - expr: kube_node_labels and on(node) kube_node_role{role="master"}
        labels:
          label_node_role_kubernetes_io: master
          label_node_role_kubernetes_io_master: "true"
        record: cluster:master_nodes
      - expr: kube_node_labels and on(node) kube_node_role{role="infra"}
        labels:
          label_node_role_kubernetes_io_infra: "true"
        record: cluster:infra_nodes
      - expr: cluster:master_nodes and on(node) cluster:infra_nodes
        labels:
          label_node_role_kubernetes_io_infra: "true"
          label_node_role_kubernetes_io_master: "true"
        record: cluster:master_infra_nodes
      - expr: cluster:master_infra_nodes or on (node) cluster:master_nodes or on (node)
          cluster:infra_nodes or on (node) kube_node_labels
        record: cluster:nodes_roles
      - expr: kube_node_labels and on(node) (sum(label_replace(node_cpu_info, "node",
          "$1", "instance", "(.*)")) by (node, package, core) == 2)
        labels:
          label_node_hyperthread_enabled: "true"
        record: cluster:hyperthread_enabled_nodes
      - expr: count(sum(virt_platform) by (instance, type)) by (type)
        record: cluster:virt_platform_nodes:sum
      - expr: sum((cluster:master_nodes * on(node) group_left kube_node_status_capacity_cpu_cores)
          or on(node) (kube_node_labels * on(node) group_left kube_node_status_capacity_cpu_cores))
          BY (label_beta_kubernetes_io_instance_type, label_node_role_kubernetes_io)
        record: cluster:capacity_cpu_cores:sum
      - expr: |
          clamp_max(
            (
              label_replace( ( ( sum (node_cpu_info) by (instance, package, core) )  > 1 ), "label_node_hyperthread_enabled", "true", "instance", "(.*)" )
              or on (instance, package)
              label_replace( ( ( sum (node_cpu_info) by (instance, package, core) ) <= 1 ), "label_node_hyperthread_enabled", "false", "instance", "(.*)" )
            ), 1
          )
        record: cluster:cpu_core_hyperthreading
      - expr: |
          cluster:nodes_roles * on (node)
            group_right( label_beta_kubernetes_io_instance_type, label_node_role_kubernetes_io, label_node_openshift_io_os_id, label_kubernetes_io_arch,
                         label_node_role_kubernetes_io_master, label_node_role_kubernetes_io_infra)
          label_replace( cluster:cpu_core_hyperthreading, "node", "$1", "instance", "(.*)" )
        record: cluster:cpu_core_node_labels
      - expr: count(cluster:cpu_core_node_labels) by (label_beta_kubernetes_io_instance_type,
          label_node_hyperthread_enabled)
        record: cluster:capacity_cpu_cores_hyperthread_enabled:sum
      - expr: sum((cluster:master_nodes * on(node) group_left kube_node_status_capacity_memory_bytes)
          or on(node) (kube_node_labels * on(node) group_left kube_node_status_capacity_memory_bytes))
          BY (label_beta_kubernetes_io_instance_type, label_node_role_kubernetes_io)
        record: cluster:capacity_memory_bytes:sum
      - expr: sum(1 - rate(node_cpu_seconds_total{mode="idle"}[2m]) * on(namespace, pod)
          group_left(node) node_namespace_pod:kube_pod_info:{})
        record: cluster:cpu_usage_cores:sum
      - expr: sum(node_memory_MemTotal_bytes{job="node-exporter"} - node_memory_MemAvailable_bytes{job="node-exporter"})
        record: cluster:memory_usage_bytes:sum
      - expr: sum(rate(container_cpu_usage_seconds_total{namespace!~"openshift-.+",pod!="",container=""}[5m]))
        record: workload:cpu_usage_cores:sum
      - expr: cluster:cpu_usage_cores:sum - workload:cpu_usage_cores:sum
        record: openshift:cpu_usage_cores:sum
      - expr: sum(container_memory_working_set_bytes{namespace!~"openshift-.+",pod!="",container=""})
        record: workload:memory_usage_bytes:sum
      - expr: cluster:memory_usage_bytes:sum - workload:memory_usage_bytes:sum
        record: openshift:memory_usage_bytes:sum
      - expr: sum(cluster:master_nodes or on(node) kube_node_labels ) BY (label_beta_kubernetes_io_instance_type,
          label_node_role_kubernetes_io)
        record: cluster:node_instance_type_count:sum
      - expr: sum(etcd_object_counts) BY (instance)
        record: instance:etcd_object_counts:sum
      - expr: max(etcd_object_counts{resource=~"builds\\.build\\.openshift\\.io|deploymentconfigs\\.apps\\.openshift\\.io|images\\.image\\.openshift\\.io|statefulsets\\.apps|deployments\\.apps|cronjobs\\.batch|jobs\\.batch|pods|persistentvolumeclaims|persistentvolumes|services|namespaces|routes\\.route\\.openshift\\.io|ingresses\\.networking\\.k8s\\.io|horizontalpodautoscalers\\.autoscaling|users\\.user\\.openshift\\.io|machinesets\\.machine\\.openshift\\.io|resourcequotas"})
          by (resource)
        record: cluster:usage:resources:sum
      - expr: count(count (kube_pod_restart_policy{type!="Always",namespace!~"openshift-.+"})
          by (namespace,pod))
        record: cluster:usage:pods:terminal:workload:sum
      - expr: sum(max(kubelet_containers_per_pod_count_sum) by (instance))
        record: cluster:usage:containers:sum
      - expr: count(cluster:cpu_core_node_labels) by (label_kubernetes_io_arch, label_node_hyperthread_enabled,
          label_node_openshift_io_os_id,label_node_role_kubernetes_io_master,label_node_role_kubernetes_io_infra)
        record: node_role_os_version_machine:cpu_capacity_cores:sum
      - expr: count(max(cluster:cpu_core_node_labels) by (node, package, label_beta_kubernetes_io_instance_type,
          label_node_hyperthread_enabled, label_node_role_kubernetes_io) ) by ( label_beta_kubernetes_io_instance_type,
          label_node_hyperthread_enabled, label_node_role_kubernetes_io)
        record: cluster:capacity_cpu_sockets_hyperthread_enabled:sum
      - expr: count (max(cluster:cpu_core_node_labels) by (node, package, label_kubernetes_io_arch,
          label_node_hyperthread_enabled, label_node_openshift_io_os_id,label_node_role_kubernetes_io_master,label_node_role_kubernetes_io_infra)
          ) by (label_kubernetes_io_arch, label_node_hyperthread_enabled, label_node_openshift_io_os_id,label_node_role_kubernetes_io_master,label_node_role_kubernetes_io_infra)
        record: node_role_os_version_machine:cpu_capacity_sockets:sum
      - alert: ClusterMonitoringOperatorReconciliationErrors
        annotations:
          message: Cluster Monitoring Operator is experiencing reconciliation error rate
            of {{ printf "%0.0f" $value }}%.
        expr: rate(cluster_monitoring_operator_reconcile_errors_total[15m]) * 100 / rate(cluster_monitoring_operator_reconcile_attempts_total[15m])
          > 10
        for: 30m
        labels:
          severity: warning
    - name: openshift-ingress.rules
      rules:
      - expr: sum by (code) (rate(haproxy_server_http_responses_total[5m]) > 0)
        record: code:cluster:ingress_http_request_count:rate5m:sum
      - expr: sum by (frontend) (rate(haproxy_frontend_bytes_in_total[5m]))
        record: frontend:cluster:ingress_frontend_bytes_in:rate5m:sum
      - expr: sum by (frontend) (rate(haproxy_frontend_bytes_out_total[5m]))
        record: frontend:cluster:ingress_frontend_bytes_out:rate5m:sum
      - expr: sum by (frontend) (haproxy_frontend_current_sessions)
        record: frontend:cluster:ingress_frontend_connections:sum
    - name: openshift-build.rules
      rules:
      - expr: sum(openshift_build_total{job="kubernetes-apiservers",phase="Error"})/(sum(openshift_build_total{job="kubernetes-apiservers",phase=~"Failed|Complete|Error"}))
        record: build_error_rate
    - name: openshift-sre.rules
      rules:
      - expr: sum(rate(apiserver_request_count{job="apiserver"}[10m])) BY (code)
        record: code:apiserver_request_count:rate:sum
      - expr: sum(rate(apiserver_request_count{job="apiserver",resource=~"image.*",verb!="WATCH"}[10m]))
          BY (code)
        record: code:registry_api_request_count:rate:sum
      - expr: sum(kube_pod_status_ready{condition="true",namespace="openshift-etcd",pod=~"etcd.*"})
          by(condition)
        record: kube_pod_status_ready:etcd:sum
      - expr: sum(kube_pod_status_ready{condition="true",namespace="openshift-image-registry",pod=~"image-registry.*"})
          by(condition)
        record: kube_pod_status_ready:image_registry:sum
    - name: node-exporter
      rules:
      - alert: NodeFilesystemSpaceFillingUp
        annotations:
          description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has
            only {{ printf "%.2f" $value }}% available space left and is filling up.
          summary: Filesystem is predicted to run out of space within the next 24 hours.
        expr: |
          (
            node_filesystem_avail_bytes{job="node-exporter",fstype!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!=""} * 100 < 40
          and
            predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype!=""}[6h], 24*60*60) < 0
          and
            node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
          )
        for: 1h
        labels:
          severity: warning
      - alert: NodeFilesystemSpaceFillingUp
        annotations:
          description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has
            only {{ printf "%.2f" $value }}% available space left and is filling up fast.
          summary: Filesystem is predicted to run out of space within the next 4 hours.
        expr: |
          (
            node_filesystem_avail_bytes{job="node-exporter",fstype!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!=""} * 100 < 20
          and
            predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype!=""}[6h], 4*60*60) < 0
          and
            node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
          )
        for: 1h
        labels:
          severity: critical
      - alert: NodeFilesystemAlmostOutOfSpace
        annotations:
          description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has
            only {{ printf "%.2f" $value }}% available space left.
          summary: Filesystem has less than 5% space left.
        expr: |
          (
            node_filesystem_avail_bytes{job="node-exporter",fstype!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!=""} * 100 < 5
          and
            node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
          )
        for: 1h
        labels:
          severity: warning
      - alert: NodeFilesystemAlmostOutOfSpace
        annotations:
          description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has
            only {{ printf "%.2f" $value }}% available space left.
          summary: Filesystem has less than 3% space left.
        expr: |
          (
            node_filesystem_avail_bytes{job="node-exporter",fstype!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!=""} * 100 < 3
          and
            node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
          )
        for: 1h
        labels:
          severity: critical
      - alert: NodeFilesystemFilesFillingUp
        annotations:
          description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has
            only {{ printf "%.2f" $value }}% available inodes left and is filling up.
          summary: Filesystem is predicted to run out of inodes within the next 24 hours.
        expr: |
          (
            node_filesystem_files_free{job="node-exporter",fstype!=""} / node_filesystem_files{job="node-exporter",fstype!=""} * 100 < 40
          and
            predict_linear(node_filesystem_files_free{job="node-exporter",fstype!=""}[6h], 24*60*60) < 0
          and
            node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
          )
        for: 1h
        labels:
          severity: warning
      - alert: NodeFilesystemFilesFillingUp
        annotations:
          description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has
            only {{ printf "%.2f" $value }}% available inodes left and is filling up fast.
          summary: Filesystem is predicted to run out of inodes within the next 4 hours.
        expr: |
          (
            node_filesystem_files_free{job="node-exporter",fstype!=""} / node_filesystem_files{job="node-exporter",fstype!=""} * 100 < 20
          and
            predict_linear(node_filesystem_files_free{job="node-exporter",fstype!=""}[6h], 4*60*60) < 0
          and
            node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
          )
        for: 1h
        labels:
          severity: critical
      - alert: NodeFilesystemAlmostOutOfFiles
        annotations:
          description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has
            only {{ printf "%.2f" $value }}% available inodes left.
          summary: Filesystem has less than 5% inodes left.
        expr: |
          (
            node_filesystem_files_free{job="node-exporter",fstype!=""} / node_filesystem_files{job="node-exporter",fstype!=""} * 100 < 5
          and
            node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
          )
        for: 1h
        labels:
          severity: warning
      - alert: NodeFilesystemAlmostOutOfFiles
        annotations:
          description: Filesystem on {{ $labels.device }} at {{ $labels.instance }} has
            only {{ printf "%.2f" $value }}% available inodes left.
          summary: Filesystem has less than 3% inodes left.
        expr: |
          (
            node_filesystem_files_free{job="node-exporter",fstype!=""} / node_filesystem_files{job="node-exporter",fstype!=""} * 100 < 3
          and
            node_filesystem_readonly{job="node-exporter",fstype!=""} == 0
          )
        for: 1h
        labels:
          severity: critical
      - alert: NodeNetworkReceiveErrs
        annotations:
          description: '{{ $labels.instance }} interface {{ $labels.device }} has encountered
            {{ printf "%.0f" $value }} receive errors in the last two minutes.'
          summary: Network interface is reporting many receive errors.
        expr: |
          increase(node_network_receive_errs_total[2m]) > 10
        for: 1h
        labels:
          severity: warning
      - alert: NodeNetworkTransmitErrs
        annotations:
          description: '{{ $labels.instance }} interface {{ $labels.device }} has encountered
            {{ printf "%.0f" $value }} transmit errors in the last two minutes.'
          summary: Network interface is reporting many transmit errors.
        expr: |
          increase(node_network_transmit_errs_total[2m]) > 10
        for: 1h
        labels:
          severity: warning
    - name: kubernetes-apps
      rules:
      - alert: KubePodCrashLooping
        annotations:
          message: Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container
            }}) is restarting {{ printf "%.2f" $value }} times / 5 minutes.
        expr: |
          rate(kube_pod_container_status_restarts_total{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"}[15m]) * 60 * 5 > 0
        for: 15m
        labels:
          severity: critical
      - alert: KubePodNotReady
        annotations:
          message: Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready
            state for longer than 15 minutes.
        expr: |
          sum by (namespace, pod) (max by(namespace, pod) (kube_pod_status_phase{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics", phase=~"Pending|Unknown"}) * on(namespace, pod) group_left(owner_kind) max by(namespace, pod, owner_kind) (kube_pod_owner{owner_kind!="Job"})) > 0
        for: 15m
        labels:
          severity: critical
      - alert: KubeDeploymentGenerationMismatch
        annotations:
          message: Deployment generation for {{ $labels.namespace }}/{{ $labels.deployment
            }} does not match, this indicates that the Deployment has failed but has not
            been rolled back.
        expr: |
          kube_deployment_status_observed_generation{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"}
            !=
          kube_deployment_metadata_generation{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"}
        for: 15m
        labels:
          severity: critical
      - alert: KubeDeploymentReplicasMismatch
        annotations:
          message: Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has not
            matched the expected number of replicas for longer than 15 minutes.
        expr: |
          kube_deployment_spec_replicas{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"}
            !=
          kube_deployment_status_replicas_available{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"}
        for: 15m
        labels:
          severity: critical
      - alert: KubeStatefulSetReplicasMismatch
        annotations:
          message: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has not
            matched the expected number of replicas for longer than 15 minutes.
        expr: |
          kube_statefulset_status_replicas_ready{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"}
            !=
          kube_statefulset_status_replicas{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"}
        for: 15m
        labels:
          severity: critical
      - alert: KubeStatefulSetGenerationMismatch
        annotations:
          message: StatefulSet generation for {{ $labels.namespace }}/{{ $labels.statefulset
            }} does not match, this indicates that the StatefulSet has failed but has
            not been rolled back.
        expr: |
          kube_statefulset_status_observed_generation{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"}
            !=
          kube_statefulset_metadata_generation{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"}
        for: 15m
        labels:
          severity: critical
      - alert: KubeStatefulSetUpdateNotRolledOut
        annotations:
          message: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} update
            has not been rolled out.
        expr: |
          max without (revision) (
            kube_statefulset_status_current_revision{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"}
              unless
            kube_statefulset_status_update_revision{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"}
          )
            *
          (
            kube_statefulset_replicas{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"}
              !=
            kube_statefulset_status_replicas_updated{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"}
          )
        for: 15m
        labels:
          severity: critical
      - alert: KubeDaemonSetRolloutStuck
        annotations:
          message: Only {{ $value | humanizePercentage }} of the desired Pods of DaemonSet
            {{ $labels.namespace }}/{{ $labels.daemonset }} are scheduled and ready.
        expr: |
          kube_daemonset_status_number_ready{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"}
            /
          kube_daemonset_status_desired_number_scheduled{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"} < 1.00
        for: 15m
        labels:
          severity: critical
      - alert: KubeContainerWaiting
        annotations:
          message: Pod {{ $labels.namespace }}/{{ $labels.pod }} container {{ $labels.container}}
            has been in waiting state for longer than 1 hour.
        expr: |
          sum by (namespace, pod, container) (kube_pod_container_status_waiting_reason{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"}) > 0
        for: 1h
        labels:
          severity: warning
      - alert: KubeDaemonSetNotScheduled
        annotations:
          message: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset
            }} are not scheduled.'
        expr: |
          kube_daemonset_status_desired_number_scheduled{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"}
            -
          kube_daemonset_status_current_number_scheduled{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"} > 0
        for: 10m
        labels:
          severity: warning
      - alert: KubeDaemonSetMisScheduled
        annotations:
          message: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset
            }} are running where they are not supposed to run.'
        expr: |
          kube_daemonset_status_number_misscheduled{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"} > 0
        for: 10m
        labels:
          severity: warning
      - alert: KubeCronJobRunning
        annotations:
          message: CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is taking more
            than 1h to complete.
        expr: |
          time() - kube_cronjob_next_schedule_time{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"} > 3600
        for: 1h
        labels:
          severity: warning
      - alert: KubeJobCompletion
        annotations:
          message: Job {{ $labels.namespace }}/{{ $labels.job_name }} is taking more than
            one hour to complete.
        expr: |
          kube_job_spec_completions{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"} - kube_job_status_succeeded{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"}  > 0
        for: 1h
        labels:
          severity: warning
      - alert: KubeJobFailed
        annotations:
          message: Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to complete.
        expr: |
          kube_job_failed{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"}  > 0
        for: 15m
        labels:
          severity: warning
      - alert: KubeHpaReplicasMismatch
        annotations:
          message: HPA {{ $labels.namespace }}/{{ $labels.hpa }} has not matched the desired
            number of replicas for longer than 15 minutes.
        expr: |
          (kube_hpa_status_desired_replicas{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"}
            !=
          kube_hpa_status_current_replicas{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"})
            and
          changes(kube_hpa_status_current_replicas[15m]) == 0
        for: 15m
        labels:
          severity: warning
      - alert: KubeHpaMaxedOut
        annotations:
          message: HPA {{ $labels.namespace }}/{{ $labels.hpa }} has been running at max
            replicas for longer than 15 minutes.
        expr: |
          kube_hpa_status_current_replicas{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"}
            ==
          kube_hpa_spec_max_replicas{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"}
        for: 15m
        labels:
          severity: warning
    - name: kubernetes-resources
      rules:
      - alert: KubeCPUOvercommit
        annotations:
          message: Cluster has overcommitted CPU resource requests for Pods and cannot
            tolerate node failure.
        expr: |
          sum(namespace:kube_pod_container_resource_requests_cpu_cores:sum)
            /
          sum(kube_node_status_allocatable_cpu_cores)
            >
          (count(kube_node_status_allocatable_cpu_cores)-1) / count(kube_node_status_allocatable_cpu_cores)
        for: 5m
        labels:
          severity: warning
      - alert: KubeMemOvercommit
        annotations:
          message: Cluster has overcommitted memory resource requests for Pods and cannot
            tolerate node failure.
        expr: |
          sum(namespace:kube_pod_container_resource_requests_memory_bytes:sum)
            /
          sum(kube_node_status_allocatable_memory_bytes)
            >
          (count(kube_node_status_allocatable_memory_bytes)-1)
            /
          count(kube_node_status_allocatable_memory_bytes)
        for: 5m
        labels:
          severity: warning
      - alert: KubeCPUOvercommit
        annotations:
          message: Cluster has overcommitted CPU resource requests for Namespaces.
        expr: |
          sum(kube_resourcequota{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics", type="hard", resource="cpu"})
            /
          sum(kube_node_status_allocatable_cpu_cores)
            > 1.5
        for: 5m
        labels:
          severity: warning
      - alert: KubeMemOvercommit
        annotations:
          message: Cluster has overcommitted memory resource requests for Namespaces.
        expr: |
          sum(kube_resourcequota{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics", type="hard", resource="memory"})
            /
          sum(kube_node_status_allocatable_memory_bytes{job="node-exporter"})
            > 1.5
        for: 5m
        labels:
          severity: warning
      - alert: KubeQuotaExceeded
        annotations:
          message: Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage
            }} of its {{ $labels.resource }} quota.
        expr: |
          kube_resourcequota{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics", type="used"}
            / ignoring(instance, job, type)
          (kube_resourcequota{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics", type="hard"} > 0)
            > 0.90
        for: 15m
        labels:
          severity: warning
      - alert: CPUThrottlingHigh
        annotations:
          message: '{{ $value | humanizePercentage }} throttling of CPU in namespace {{
            $labels.namespace }} for container {{ $labels.container }} in pod {{ $labels.pod
            }}.'
        expr: |
          sum(increase(container_cpu_cfs_throttled_periods_total{container!="", }[5m])) by (container, pod, namespace)
            /
          sum(increase(container_cpu_cfs_periods_total{}[5m])) by (container, pod, namespace)
            > ( 25 / 100 )
        for: 15m
        labels:
          severity: warning
    - name: kubernetes-storage
      rules:
      - alert: KubePersistentVolumeUsageCritical
        annotations:
          message: The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }}
            in Namespace {{ $labels.namespace }} is only {{ $value | humanizePercentage
            }} free.
        expr: |
          kubelet_volume_stats_available_bytes{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kubelet"}
            /
          kubelet_volume_stats_capacity_bytes{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kubelet"}
            < 0.03
        for: 1m
        labels:
          severity: critical
      - alert: KubePersistentVolumeFullInFourDays
        annotations:
          message: Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim
            }} in Namespace {{ $labels.namespace }} is expected to fill up within four
            days. Currently {{ $value | humanizePercentage }} is available.
        expr: |
          (
            kubelet_volume_stats_available_bytes{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kubelet"}
              /
            kubelet_volume_stats_capacity_bytes{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kubelet"}
          ) < 0.15
          and
          predict_linear(kubelet_volume_stats_available_bytes{namespace=~"(openshift-.*|kube-.*|default|logging)",job="kubelet"}[6h], 4 * 24 * 3600) < 0
        for: 1h
        labels:
          severity: critical
      - alert: KubePersistentVolumeErrors
        annotations:
          message: The persistent volume {{ $labels.persistentvolume }} has status {{
            $labels.phase }}.
        expr: |
          kube_persistentvolume_status_phase{phase=~"Failed|Pending",namespace=~"(openshift-.*|kube-.*|default|logging)",job="kube-state-metrics"} > 0
        for: 5m
        labels:
          severity: critical
    - name: kubernetes-system
      rules:
      - alert: KubeVersionMismatch
        annotations:
          message: There are {{ $value }} different semantic versions of Kubernetes components
            running.
        expr: |
          count(count by (gitVersion) (label_replace(kubernetes_build_info{job!~"kube-dns|coredns"},"gitVersion","$1","gitVersion","(v[0-9]*.[0-9]*.[0-9]*).*"))) > 1
        for: 15m
        labels:
          severity: warning
      - alert: KubeClientErrors
        annotations:
          message: Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance
            }}' is experiencing {{ $value | humanizePercentage }} errors.'
        expr: |
          (sum(rate(rest_client_requests_total{code=~"5.."}[5m])) by (instance, job)
            /
          sum(rate(rest_client_requests_total[5m])) by (instance, job))
          > 0.01
        for: 15m
        labels:
          severity: warning
    - name: kubernetes-system-apiserver
      rules:
      - alert: KubeAPILatencyHigh
        annotations:
          message: The API server has an abnormal latency of {{ $value }} seconds for
            {{ $labels.verb }} {{ $labels.resource }}.
        expr: |
          (
            cluster:apiserver_request_duration_seconds:mean5m{job="apiserver"}
            >
            on (verb) group_left()
            (
              avg by (verb) (cluster:apiserver_request_duration_seconds:mean5m{job="apiserver"} >= 0)
              +
              2*stddev by (verb) (cluster:apiserver_request_duration_seconds:mean5m{job="apiserver"} >= 0)
            )
          ) > on (verb) group_left()
          1.2 * avg by (verb) (cluster:apiserver_request_duration_seconds:mean5m{job="apiserver"} >= 0)
          and on (verb,resource)
          cluster_quantile:apiserver_request_duration_seconds:histogram_quantile{job="apiserver",quantile="0.99"}
          >
          1
        for: 5m
        labels:
          severity: warning
      - alert: KubeAPILatencyHigh
        annotations:
          message: The API server has a 99th percentile latency of {{ $value }} seconds
            for {{ $labels.verb }} {{ $labels.resource }}.
        expr: |
          cluster_quantile:apiserver_request_duration_seconds:histogram_quantile{job="apiserver",quantile="0.99"} > 4
        for: 10m
        labels:
          severity: critical
      - alert: KubeAPIErrorsHigh
        annotations:
          message: API server is returning errors for {{ $value | humanizePercentage }}
            of requests.
        expr: |
          sum(rate(apiserver_request_total{job="apiserver",code=~"5.."}[5m]))
            /
          sum(rate(apiserver_request_total{job="apiserver"}[5m])) > 0.03
        for: 10m
        labels:
          severity: critical
      - alert: KubeAPIErrorsHigh
        annotations:
          message: API server is returning errors for {{ $value | humanizePercentage }}
            of requests.
        expr: |
          sum(rate(apiserver_request_total{job="apiserver",code=~"5.."}[5m]))
            /
          sum(rate(apiserver_request_total{job="apiserver"}[5m])) > 0.01
        for: 10m
        labels:
          severity: warning
      - alert: KubeAPIErrorsHigh
        annotations:
          message: API server is returning errors for {{ $value | humanizePercentage }}
            of requests for {{ $labels.verb }} {{ $labels.resource }} {{ $labels.subresource
            }}.
        expr: |
          sum(rate(apiserver_request_total{job="apiserver",code=~"5.."}[5m])) by (resource,subresource,verb)
            /
          sum(rate(apiserver_request_total{job="apiserver"}[5m])) by (resource,subresource,verb) > 0.10
        for: 10m
        labels:
          severity: critical
      - alert: KubeAPIErrorsHigh
        annotations:
          message: API server is returning errors for {{ $value | humanizePercentage }}
            of requests for {{ $labels.verb }} {{ $labels.resource }} {{ $labels.subresource
            }}.
        expr: |
          sum(rate(apiserver_request_total{job="apiserver",code=~"5.."}[5m])) by (resource,subresource,verb)
            /
          sum(rate(apiserver_request_total{job="apiserver"}[5m])) by (resource,subresource,verb) > 0.05
        for: 10m
        labels:
          severity: warning
      - alert: KubeClientCertificateExpiration
        annotations:
          message: A client certificate used to authenticate to the apiserver is expiring
            in less than 1.5 hours.
        expr: |
          apiserver_client_certificate_expiration_seconds_count{job="apiserver"} > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 5400
        labels:
          severity: warning
      - alert: KubeClientCertificateExpiration
        annotations:
          message: A client certificate used to authenticate to the apiserver is expiring
            in less than 1.0 hours.
        expr: |
          apiserver_client_certificate_expiration_seconds_count{job="apiserver"} > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 3600
        labels:
          severity: critical
      - alert: KubeAPIDown
        annotations:
          message: KubeAPI has disappeared from Prometheus target discovery.
        expr: |
          absent(up{job="apiserver"} == 1)
        for: 15m
        labels:
          severity: critical
    - name: kubernetes-system-kubelet
      rules:
      - alert: KubeNodeNotReady
        annotations:
          message: '{{ $labels.node }} has been unready for more than 15 minutes.'
        expr: |
          kube_node_status_condition{job="kube-state-metrics",condition="Ready",status="true"} == 0
        for: 15m
        labels:
          severity: warning
      - alert: KubeNodeUnreachable
        annotations:
          message: '{{ $labels.node }} is unreachable and some workloads may be rescheduled.'
        expr: |
          kube_node_spec_taint{job="kube-state-metrics",key="node.kubernetes.io/unreachable",effect="NoSchedule"} == 1
        labels:
          severity: warning
      - alert: KubeletTooManyPods
        annotations:
          message: Kubelet '{{ $labels.node }}' is running at {{ $value | humanizePercentage
            }} of its Pod capacity.
        expr: |
          max(max(kubelet_running_pod_count{job="kubelet"}) by(instance) * on(instance) group_left(node) kubelet_node_name{job="kubelet"}) by(node) / max(kube_node_status_capacity_pods{job="kube-state-metrics"}) by(node) > 0.95
        for: 15m
        labels:
          severity: warning
      - alert: KubeletDown
        annotations:
          message: Kubelet has disappeared from Prometheus target discovery.
        expr: |
          absent(up{job="kubelet"} == 1)
        for: 15m
        labels:
          severity: critical
    - name: kubernetes-system-scheduler
      rules:
      - alert: KubeSchedulerDown
        annotations:
          message: KubeScheduler has disappeared from Prometheus target discovery.
        expr: |
          absent(up{job="scheduler"} == 1)
        for: 15m
        labels:
          severity: critical
    - name: kubernetes-system-controller-manager
      rules:
      - alert: KubeControllerManagerDown
        annotations:
          message: KubeControllerManager has disappeared from Prometheus target discovery.
        expr: |
          absent(up{job="kube-controller-manager"} == 1)
        for: 15m
        labels:
          severity: critical
    - name: prometheus
      rules:
      - alert: PrometheusBadConfig
        annotations:
          description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to
            reload its configuration.
          summary: Failed Prometheus configuration reload.
        expr: |
          # Without max_over_time, failed scrapes could create false negatives, see
          # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
          max_over_time(prometheus_config_last_reload_successful{job="prometheus-k8s",namespace="openshift-monitoring"}[5m]) == 0
        for: 10m
        labels:
          severity: critical
      - alert: PrometheusNotificationQueueRunningFull
        annotations:
          description: Alert notification queue of Prometheus {{$labels.namespace}}/{{$labels.pod}}
            is running full.
          summary: Prometheus alert notification queue predicted to run full in less than
            30m.
        expr: |
          # Without min_over_time, failed scrapes could create false negatives, see
          # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
          (
            predict_linear(prometheus_notifications_queue_length{job="prometheus-k8s",namespace="openshift-monitoring"}[5m], 60 * 30)
          >
            min_over_time(prometheus_notifications_queue_capacity{job="prometheus-k8s",namespace="openshift-monitoring"}[5m])
          )
        for: 15m
        labels:
          severity: warning
      - alert: PrometheusErrorSendingAlertsToSomeAlertmanagers
        annotations:
          description: '{{ printf "%.1f" $value }}% errors while sending alerts from Prometheus
            {{$labels.namespace}}/{{$labels.pod}} to Alertmanager {{$labels.alertmanager}}.'
          summary: Prometheus has encountered more than 1% errors sending alerts to a
            specific Alertmanager.
        expr: |
          (
            rate(prometheus_notifications_errors_total{job="prometheus-k8s",namespace="openshift-monitoring"}[5m])
          /
            rate(prometheus_notifications_sent_total{job="prometheus-k8s",namespace="openshift-monitoring"}[5m])
          )
          * 100
          > 1
        for: 15m
        labels:
          severity: warning
      - alert: PrometheusErrorSendingAlertsToAnyAlertmanager
        annotations:
          description: '{{ printf "%.1f" $value }}% minimum errors while sending alerts
            from Prometheus {{$labels.namespace}}/{{$labels.pod}} to any Alertmanager.'
          summary: Prometheus encounters more than 3% errors sending alerts to any Alertmanager.
        expr: |
          min without(alertmanager) (
            rate(prometheus_notifications_errors_total{job="prometheus-k8s",namespace="openshift-monitoring"}[5m])
          /
            rate(prometheus_notifications_sent_total{job="prometheus-k8s",namespace="openshift-monitoring"}[5m])
          )
          * 100
          > 3
        for: 15m
        labels:
          severity: critical
      - alert: PrometheusNotConnectedToAlertmanagers
        annotations:
          description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is not connected
            to any Alertmanagers.
          summary: Prometheus is not connected to any Alertmanagers.
        expr: |
          # Without max_over_time, failed scrapes could create false negatives, see
          # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
          max_over_time(prometheus_notifications_alertmanagers_discovered{job="prometheus-k8s",namespace="openshift-monitoring"}[5m]) < 1
        for: 10m
        labels:
          severity: warning
      - alert: PrometheusTSDBReloadsFailing
        annotations:
          description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected {{$value
            | humanize}} reload failures over the last 3h.
          summary: Prometheus has issues reloading blocks from disk.
        expr: |
          increase(prometheus_tsdb_reloads_failures_total{job="prometheus-k8s",namespace="openshift-monitoring"}[3h]) > 0
        for: 4h
        labels:
          severity: warning
      - alert: PrometheusTSDBCompactionsFailing
        annotations:
          description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected {{$value
            | humanize}} compaction failures over the last 3h.
          summary: Prometheus has issues compacting blocks.
        expr: |
          increase(prometheus_tsdb_compactions_failed_total{job="prometheus-k8s",namespace="openshift-monitoring"}[3h]) > 0
        for: 4h
        labels:
          severity: warning
      - alert: PrometheusNotIngestingSamples
        annotations:
          description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is not ingesting
            samples.
          summary: Prometheus is not ingesting samples.
        expr: |
          rate(prometheus_tsdb_head_samples_appended_total{job="prometheus-k8s",namespace="openshift-monitoring"}[5m]) <= 0
        for: 10m
        labels:
          severity: warning
      - alert: PrometheusDuplicateTimestamps
        annotations:
          description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping {{
            printf "%.4g" $value  }} samples/s with different values but duplicated timestamp.
          summary: Prometheus is dropping samples with duplicate timestamps.
        expr: |
          rate(prometheus_target_scrapes_sample_duplicate_timestamp_total{job="prometheus-k8s",namespace="openshift-monitoring"}[5m]) > 0
        for: 1h
        labels:
          severity: warning
      - alert: PrometheusOutOfOrderTimestamps
        annotations:
          description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping {{
            printf "%.4g" $value  }} samples/s with timestamps arriving out of order.
          summary: Prometheus drops samples with out-of-order timestamps.
        expr: |
          rate(prometheus_target_scrapes_sample_out_of_order_total{job="prometheus-k8s",namespace="openshift-monitoring"}[5m]) > 0
        for: 1h
        labels:
          severity: warning
      - alert: PrometheusRemoteStorageFailures
        annotations:
          description: Prometheus {{$labels.namespace}}/{{$labels.pod}} failed to send
            {{ printf "%.1f" $value }}% of the samples to queue {{$labels.queue}}.
          summary: Prometheus fails to send samples to remote storage.
        expr: |
          (
            rate(prometheus_remote_storage_failed_samples_total{job="prometheus-k8s",namespace="openshift-monitoring"}[5m])
          /
            (
              rate(prometheus_remote_storage_failed_samples_total{job="prometheus-k8s",namespace="openshift-monitoring"}[5m])
            +
              rate(prometheus_remote_storage_succeeded_samples_total{job="prometheus-k8s",namespace="openshift-monitoring"}[5m])
            )
          )
          * 100
          > 1
        for: 15m
        labels:
          severity: critical
      - alert: PrometheusRemoteWriteBehind
        annotations:
          description: Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write is
            {{ printf "%.1f" $value }}s behind for queue {{$labels.queue}}.
          summary: Prometheus remote write is behind.
        expr: |
          # Without max_over_time, failed scrapes could create false negatives, see
          # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
          (
            max_over_time(prometheus_remote_storage_highest_timestamp_in_seconds{job="prometheus-k8s",namespace="openshift-monitoring"}[5m])
          - on(job, instance) group_right
            max_over_time(prometheus_remote_storage_queue_highest_sent_timestamp_seconds{job="prometheus-k8s",namespace="openshift-monitoring"}[5m])
          )
          > 120
        for: 15m
        labels:
          severity: critical
      - alert: PrometheusRemoteWriteDesiredShards
        annotations:
          description: Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write desired
            shards calculation wants to run {{ $value }} shards, which is more than the
            max of {{ printf `prometheus_remote_storage_shards_max{instance="%s",job="prometheus-k8s",namespace="openshift-monitoring"}`
            $labels.instance | query | first | value }}.
          summary: Prometheus remote write desired shards calculation wants to run more
            than configured max shards.
        expr: |
          # Without max_over_time, failed scrapes could create false negatives, see
          # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
          (
            max_over_time(prometheus_remote_storage_shards_desired{job="prometheus-k8s",namespace="openshift-monitoring"}[5m])
          >
            max_over_time(prometheus_remote_storage_shards_max{job="prometheus-k8s",namespace="openshift-monitoring"}[5m])
          )
        for: 15m
        labels:
          severity: warning
      - alert: PrometheusRuleFailures
        annotations:
          description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to
            evaluate {{ printf "%.0f" $value }} rules in the last 5m.
          summary: Prometheus is failing rule evaluations.
        expr: |
          increase(prometheus_rule_evaluation_failures_total{job="prometheus-k8s",namespace="openshift-monitoring"}[5m]) > 0
        for: 15m
        labels:
          severity: critical
      - alert: PrometheusMissingRuleEvaluations
        annotations:
          description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has missed {{
            printf "%.0f" $value }} rule group evaluations in the last 5m.
          summary: Prometheus is missing rule evaluations due to slow rule group evaluation.
        expr: |
          increase(prometheus_rule_group_iterations_missed_total{job="prometheus-k8s",namespace="openshift-monitoring"}[5m]) > 0
        for: 15m
        labels:
          severity: warning
    - name: alertmanager.rules
      rules:
      - alert: AlertmanagerConfigInconsistent
        annotations:
          message: The configuration of the instances of the Alertmanager cluster `{{$labels.service}}`
            are out of sync.
        expr: |
          count_values("config_hash", alertmanager_config_hash{job="alertmanager-main",namespace="openshift-monitoring"}) BY (service) / ON(service) GROUP_LEFT() label_replace(max(prometheus_operator_spec_replicas{job="prometheus-operator",namespace="openshift-monitoring",controller="alertmanager"}) by (name, job, namespace, controller), "service", "alertmanager-$1", "name", "(.*)") != 1
        for: 5m
        labels:
          severity: critical
      - alert: AlertmanagerFailedReload
        annotations:
          message: Reloading Alertmanager's configuration has failed for {{ $labels.namespace
            }}/{{ $labels.pod}}.
        expr: |
          alertmanager_config_last_reload_successful{job="alertmanager-main",namespace="openshift-monitoring"} == 0
        for: 10m
        labels:
          severity: warning
      - alert: AlertmanagerMembersInconsistent
        annotations:
          message: Alertmanager has not found all other members of the cluster.
        expr: |
          alertmanager_cluster_members{job="alertmanager-main",namespace="openshift-monitoring"}
            != on (service) GROUP_LEFT()
          count by (service) (alertmanager_cluster_members{job="alertmanager-main",namespace="openshift-monitoring"})
        for: 5m
        labels:
          severity: critical
    - name: general.rules
      rules:
      - alert: TargetDown
        annotations:
          message: '{{ printf "%.4g" $value }}% of the {{ $labels.job }} targets in {{
            $labels.namespace }} namespace are down.'
        expr: 100 * (count(up == 0) BY (job, namespace, service) / count(up) BY (job,
          namespace, service)) > 10
        for: 10m
        labels:
          severity: warning
      - alert: Watchdog
        annotations:
          message: |
            This is an alert meant to ensure that the entire alerting pipeline is functional.
            This alert is always firing, therefore it should always be firing in Alertmanager
            and always fire against a receiver. There are integrations with various notification
            mechanisms that send a notification when this alert is not firing. For example the
            "DeadMansSnitch" integration in PagerDuty.
        expr: vector(1)
        labels:
          severity: none
    - name: node-time
      rules:
      - alert: ClockSkewDetected
        annotations:
          message: Clock skew detected on node-exporter {{ $labels.namespace }}/{{ $labels.pod
            }}. Ensure NTP is configured correctly on this host.
        expr: |
          abs(node_timex_offset_seconds{job="node-exporter"}) > 0.05
        for: 2m
        labels:
          severity: warning
    - name: node-network
      rules:
      - alert: NodeNetworkInterfaceFlapping
        annotations:
          message: Network interface "{{ $labels.device }}" changing it's up status often
            on node-exporter {{ $labels.namespace }}/{{ $labels.pod }}"
        expr: |
          changes(node_network_up{job="node-exporter",device!~"veth.+"}[2m]) > 2
        for: 2m
        labels:
          severity: warning
    - name: prometheus-operator
      rules:
      - alert: PrometheusOperatorReconcileErrors
        annotations:
          message: Errors while reconciling {{ $labels.controller }} in {{ $labels.namespace
            }} Namespace.
        expr: |
          rate(prometheus_operator_reconcile_errors_total{job="prometheus-operator",namespace="openshift-monitoring"}[5m]) > 0.1
        for: 10m
        labels:
          severity: warning
      - alert: PrometheusOperatorNodeLookupErrors
        annotations:
          message: Errors while reconciling Prometheus in {{ $labels.namespace }} Namespace.
        expr: |
          rate(prometheus_operator_node_address_lookup_errors_total{job="prometheus-operator",namespace="openshift-monitoring"}[5m]) > 0.1
        for: 10m
        labels:
          severity: warning
    - name: etcd
      rules:
      - alert: etcdMembersDown
        annotations:
          message: 'etcd cluster "{{ $labels.job }}": members are down ({{ $value }}).'
        expr: |
          max by (job) (
            sum by (job) (up{job=~".*etcd.*"} == bool 0)
          or
            count by (job,endpoint) (
              sum by (job,endpoint,To) (rate(etcd_network_peer_sent_failures_total{job=~".*etcd.*"}[3m])) > 0.01
            )
          )
          > 0
        for: 3m
        labels:
          severity: critical
      - alert: etcdInsufficientMembers
        annotations:
          message: 'etcd cluster "{{ $labels.job }}": insufficient members ({{ $value
            }}).'
        expr: |
          sum(up{job=~".*etcd.*"} == bool 1) by (job) < ((count(up{job=~".*etcd.*"}) by (job) + 1) / 2)
        for: 3m
        labels:
          severity: critical
      - alert: etcdNoLeader
        annotations:
          message: 'etcd cluster "{{ $labels.job }}": member {{ $labels.instance }} has
            no leader.'
        expr: |
          etcd_server_has_leader{job=~".*etcd.*"} == 0
        for: 1m
        labels:
          severity: critical
      - alert: etcdHighNumberOfLeaderChanges
        annotations:
          message: 'etcd cluster "{{ $labels.job }}": {{ $value }} leader changes within
            the last 15 minutes. Frequent elections may be a sign of insufficient resources,
            high network latency, or disruptions by other components and should be investigated.'
        expr: |
          increase((max by (job) (etcd_server_leader_changes_seen_total{job=~".*etcd.*"}) or 0*absent(etcd_server_leader_changes_seen_total{job=~".*etcd.*"}))[15m:1m]) >= 3
        for: 5m
        labels:
          severity: warning
      - alert: etcdGRPCRequestsSlow
        annotations:
          message: 'etcd cluster "{{ $labels.job }}": gRPC requests to {{ $labels.grpc_method
            }} are taking {{ $value }}s on etcd instance {{ $labels.instance }}.'
        expr: |
          histogram_quantile(0.99, sum(rate(grpc_server_handling_seconds_bucket{job=~".*etcd.*", grpc_type="unary"}[5m])) by (job, instance, grpc_service, grpc_method, le))
          > 0.15
        for: 10m
        labels:
          severity: critical
      - alert: etcdMemberCommunicationSlow
        annotations:
          message: 'etcd cluster "{{ $labels.job }}": member communication with {{ $labels.To
            }} is taking {{ $value }}s on etcd instance {{ $labels.instance }}.'
        expr: |
          histogram_quantile(0.99, rate(etcd_network_peer_round_trip_time_seconds_bucket{job=~".*etcd.*"}[5m]))
          > 0.15
        for: 10m
        labels:
          severity: warning
      - alert: etcdHighNumberOfFailedProposals
        annotations:
          message: 'etcd cluster "{{ $labels.job }}": {{ $value }} proposal failures within
            the last 30 minutes on etcd instance {{ $labels.instance }}.'
        expr: |
          rate(etcd_server_proposals_failed_total{job=~".*etcd.*"}[15m]) > 5
        for: 15m
        labels:
          severity: warning
      - alert: etcdHighFsyncDurations
        annotations:
          message: 'etcd cluster "{{ $labels.job }}": 99th percentile fync durations are
            {{ $value }}s on etcd instance {{ $labels.instance }}.'
        expr: |
          histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket{job=~".*etcd.*"}[5m]))
          > 0.5
        for: 10m
        labels:
          severity: warning
      - alert: etcdHighCommitDurations
        annotations:
          message: 'etcd cluster "{{ $labels.job }}": 99th percentile commit durations
            {{ $value }}s on etcd instance {{ $labels.instance }}.'
        expr: |
          histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket{job=~".*etcd.*"}[5m]))
          > 0.25
        for: 10m
        labels:
          severity: warning
      - alert: etcdHighNumberOfFailedHTTPRequests
        annotations:
          message: '{{ $value }}% of requests for {{ $labels.method }} failed on etcd
            instance {{ $labels.instance }}'
        expr: |
          sum(rate(etcd_http_failed_total{job=~".*etcd.*", code!="404"}[5m])) BY (method) / sum(rate(etcd_http_received_total{job=~".*etcd.*"}[5m]))
          BY (method) > 0.01
        for: 10m
        labels:
          severity: warning
      - alert: etcdHighNumberOfFailedHTTPRequests
        annotations:
          message: '{{ $value }}% of requests for {{ $labels.method }} failed on etcd
            instance {{ $labels.instance }}.'
        expr: |
          sum(rate(etcd_http_failed_total{job=~".*etcd.*", code!="404"}[5m])) BY (method) / sum(rate(etcd_http_received_total{job=~".*etcd.*"}[5m]))
          BY (method) > 0.05
        for: 10m
        labels:
          severity: critical
      - alert: etcdHTTPRequestsSlow
        annotations:
          message: etcd instance {{ $labels.instance }} HTTP requests to {{ $labels.method
            }} are slow.
        expr: |
          histogram_quantile(0.99, rate(etcd_http_successful_duration_seconds_bucket[5m]))
          > 0.15
        for: 10m
        labels:
          severity: warning
  openshift-multus-prometheus-k8s-rules.yaml: |
    groups:
    - name: multus-admission-controller-monitor-service.rules
      rules:
      - expr: |
          max  (network_attachment_definition_enabled_instance_up) by (networks)
        record: cluster:network_attachment_definition_enabled_instance_up:max
      - expr: |
          max  (network_attachment_definition_instances) by (networks)
        record: cluster:network_attachment_definition_instances:max
  openshift-operator-lifecycle-manager-olm-alert-rules.yaml: |
    groups:
    - name: olm.failing_operators.rules
      rules:
      - alert: FailingOperator
        annotations:
          message: Failed to install Operator {{ $labels.name }} version {{ $labels.version
            }}. Reason-{{ $labels.reason }}
        expr: csv_abnormal{phase="Failed"}
        labels:
          severity: info
  openshift-sdn-networking-rules.yaml: |
    groups:
    - name: general.rules
      rules:
      - alert: NodeWithoutOVSPod
        annotations:
          message: |
            All nodes should be running an ovs pod, {{ $labels.node }} is not.
        expr: |
          (kube_node_info unless on(node) kube_pod_info{namespace="openshift-sdn",  pod=~"ovs.*"}) > 0
        for: 20m
        labels:
          severity: warning
      - alert: NodeWithoutSDNPod
        annotations:
          message: |
            All nodes should be running an sdn pod, {{ $labels.node }} is not.
        expr: |
          (kube_node_info unless on(node) kube_pod_info{namespace="openshift-sdn",  pod=~"sdn.*"}) > 0
        for: 20m
        labels:
          severity: warning
      - alert: NetworkPodsCrashLooping
        annotations:
          message: Pod {{ $labels.namespace}}/{{ $labels.pod}} ({{ $labels.container }})
            is restarting {{ printf "%.2f" $value }} times / 5 minutes.
        expr: |
          rate(kube_pod_container_status_restarts_total{namespace="openshift-sdn"}[15m]) * 60 * 5 > 0
        for: 1h
        labels:
          severity: warning
      - alert: IPTableSyncSDNPod
        annotations:
          message: SDN pod {{ $labels.pod }} on node {{ $labels.node }} takes too long
            to sync iptables rules.
        expr: |
          histogram_quantile(.95, kubeproxy_sync_proxy_rules_duration_seconds_bucket) * on(pod) group_right kube_pod_info{namespace="openshift-sdn",  pod=~"sdn-[^-]*"} > 15
        labels:
          severity: warning
      - alert: IPTableSyncCluster
        annotations:
          message: The average time for SDN pods to sync iptables is too high.
        expr: |
          histogram_quantile(0.95, sum(rate(kubeproxy_sync_proxy_rules_duration_seconds_bucket[5m])) by (le)) > 10
        labels:
          severity: warning
      - alert: NodeIPTablesStale
        annotations:
          message: SDN pod {{ $labels.pod }} on node {{ $labels.node }} has gone too long
            without syncing iptables rules.
        expr: |
          (timestamp(kubeproxy_sync_proxy_rules_last_timestamp_seconds)
          - on(pod) kubeproxy_sync_proxy_rules_last_timestamp_seconds)
          * on(pod) group_right kube_pod_info{namespace="openshift-sdn",  pod=~"sdn-[^-]*"}> 120
        for: 20m
        labels:
          severity: warning
      - alert: ClusterIPTablesStale
        annotations:
          message: The average time between iptables resyncs is too high. NOTE - There
            is some scrape delay and other offsets, 90s isn't exact but it is still too
            high.
        expr: |
          quantile(0.95,
              timestamp(kubeproxy_sync_proxy_rules_last_timestamp_seconds)
              - on(pod) kubeproxy_sync_proxy_rules_last_timestamp_seconds
              * on(pod) group_right kube_pod_info{namespace="openshift-sdn",  pod=~"sdn-[^-]*"}) > 90
        for: 20m
        labels:
          severity: warning
  openshift-service-catalog-apiserver-operator-openshift-service-catalog-apiserver-operator.yaml: |
    groups:
    - name: svcat-apiserver-enabled
      rules:
      - alert: ServiceCatalogAPIServerEnabled
        annotations:
          message: Indicates whether Service Catalog API Server is enabled
          summary: Indicates whether Service Catalog API Server is enabled
        expr: |
          service_catalog_apiserver_enabled > 0
        labels:
          severity: warning
  openshift-service-catalog-controller-manager-operator-openshift-service-catalog-controller-manager-operator.yaml: |
    groups:
    - name: svcat-controller-manager-enabled
      rules:
      - alert: ServiceCatalogControllerManagerEnabled
        annotations:
          message: Indicates whether Service Catalog Controller Manager is enabled
          summary: Indicates whether Service Catalog Controller Manager is enabled
        expr: |
          service_catalog_controller_manager_enabled > 0
        labels:
          severity: warning
kind: ConfigMap
metadata:
  creationTimestamp: "2020-05-07T17:29:54Z"
  labels:
    managed-by: prometheus-operator
    prometheus-name: k8s
  name: prometheus-k8s-rulefiles-0
  namespace: openshift-monitoring
  ownerReferences:
  - apiVersion: monitoring.coreos.com/v1
    blockOwnerDeletion: true
    controller: true
    kind: Prometheus
    name: k8s
    uid: c1a559b4-c36e-48d5-be02-c2179dafd9e9
  resourceVersion: "12429990"
  selfLink: /api/v1/namespaces/openshift-monitoring/configmaps/prometheus-k8s-rulefiles-0
  uid: 657e8283-9c68-400f-a57c-afc0df0812b9
